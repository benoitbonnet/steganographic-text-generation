{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Génération de Texte sous contrainte stéganographiqe</h1>\n",
    "\n",
    "Dans ce notebook, nous étudions le codage arithmétique comme méthode d'insertion stéganographique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import bitarray\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation des différentes variables et nom du modèle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_secret = \"I am hidden\"\n",
    "message_secret = message_secret + '<eos>'\n",
    "\n",
    "amorce = \"I am going on a vacation to Italy. I am hoping that my\"\n",
    "\n",
    "model_name = \"gpt2-large\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger modèle et tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodage du message et de l'amorce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amorce: I am going on a vacation to Italy. I am hoping that my \n",
      "encodée en: [40, 716, 1016, 319, 257, 14600, 284, 8031, 13, 314, 716, 7725, 326, 616]\n",
      "\n",
      "message_secret: I am hidden<eos> \n",
      "encodée en: [40, 716, 7104, 27, 68, 418, 29]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_message = tokenizer.encode(message_secret)\n",
    "encoded_context = tokenizer.encode(amorce)\n",
    "\n",
    "print(\"amorce: {} \\nencodée en: {}\\n\".format(amorce, encoded_context))\n",
    "print(\"message_secret: {} \\nencodée en: {}\\n\".format(message_secret, encoded_message))\n",
    "\n",
    "tensor_amorce = torch.LongTensor(encoded_context).view(1,-1)\n",
    "tensor_message = torch.LongTensor(encoded_message).view(1,-1)\n",
    "\n",
    "#liste des tokens visibles sur:\n",
    "#https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple de séquences générées sans contrainte stéga:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exemple 0: I am going on a vacation to Italy. I am hoping that my wife will not die before I see her, and that her death will be in one or two days, and will certainly be a\n",
      "\n",
      "exemple 1: I am going on a vacation to Italy. I am hoping that my new place will be a great place to go as I have found no places that suit me.\n",
      "\n",
      "I am also in the\n",
      "\n",
      "exemple 2: I am going on a vacation to Italy. I am hoping that my dad comes on board. I have so many questions, but I am going to send you a quick email before I leave.\" I\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_output = model.generate(tensor_amorce, do_sample=True, max_length=40,\\\n",
    "                                 top_k=50, top_p=1, temperature=1, num_return_sequences=3)\n",
    "\n",
    "for i in range(sampling_output.shape[0]):\n",
    "        print(\"exemple {}: {}\\n\".format(i,tokenizer.decode(sampling_output[i].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Déclaration de fonctions utiles pour la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def int2bits(inp, num_bits):\n",
    "    if num_bits == 0:\n",
    "        return []\n",
    "    strlist = ('{0:0%db}' % num_bits).format(inp)\n",
    "    return [int(strval) for strval in reversed(strlist)]\n",
    "\n",
    "def num_same_from_beg(bits1, bits2):\n",
    "    assert len(bits1) == len(bits2)\n",
    "    for i in range(len(bits1)):\n",
    "        if bits1[i] != bits2[i]:\n",
    "            break\n",
    "\n",
    "    return i\n",
    "\n",
    "def bits2int(bits):\n",
    "    res = 0\n",
    "    for i, bit in enumerate(bits):\n",
    "        res += bit * (2 ** i)\n",
    "    return res\n",
    "\n",
    "\n",
    "def str2bit(msg_str, tokenizer, model, context=None, topk=60000):\n",
    "    if context is None:\n",
    "        message_ctx = tokenizer.encode('<|endoftext|>')\n",
    "    else: message_ctx = tokenizer.encode(context)\n",
    "    msg_str += '<eos>'\n",
    "    msg_bits = bitarray.bitarray()\n",
    "    msg_enc = encode_arithmetic(model, tokenizer, msg_str, message_ctx,\n",
    "                                    precision=40, topk=topk, device='cpu')\n",
    "    msg_bits = bitarray.bitarray(msg_enc)\n",
    "        \n",
    "    return msg_bits\n",
    "\n",
    "def bit2str(msg_bits, tokenizer, model, context=None, topk=60000, finish_sent=False):\n",
    "    \n",
    "    if context is None:\n",
    "        message_ctx = tokenizer.encode('<|endoftext|>')\n",
    "    else: \n",
    "        message_ctx = tokenizer.encode(context)\n",
    "        \n",
    "    msg_str = decode_arithmetic(model, tokenizer, msg_bits, message_ctx,\n",
    "        precision=40, topk=topk, device=\"cpu\", model_device='cpu', finish_sent=finish_sent)\n",
    "    msg_str = tokenizer.decode(msg_str)\n",
    "    return msg_str\n",
    "\n",
    "\n",
    "def is_sent_finish(token_idx, enc):\n",
    "    token = enc.decode(token_idx)\n",
    "    return '.' in token or '!' in token or '?' in token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code pour l'encodage arithmétique (de texte à bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_arithmetic(model, enc, text, context, device='cuda', temp=1.0, precision=16, topk=50000):\n",
    "    # inp is a list of token indices\n",
    "    # context is a list of token indices\n",
    "    inp = enc.encode(text)\n",
    "    # common BPE error case: 128, 128 (2 newlines) is interpretted as 628 (2 newlines)\n",
    "    i = 0\n",
    "    while i < len(inp):\n",
    "        if inp[i] == 628:\n",
    "            inp[i] = 198\n",
    "            inp[i + 1:i + 1] = [198]\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    context = torch.tensor(context[-1022:], device=device, dtype=torch.long)\n",
    "\n",
    "    max_val = 2 ** precision\n",
    "    threshold = 2 ** (-precision)\n",
    "    cur_interval = [0, max_val]  # bottom inclusive, top exclusive\n",
    "\n",
    "    prev = context\n",
    "    past = None\n",
    "    message = []\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        while i < len(inp):\n",
    "            outputs = model(prev.unsqueeze(0), past_key_values=past)\n",
    "            past, logits = outputs.past_key_values, outputs.logits\n",
    "            \n",
    "            logits[0, -1, -1] = -1e10  # endoftext can't happen\n",
    "            logits[0, -1, 628] = -1e10  # 2 newlines can't happen\n",
    "            logits, indices = logits[0, -1, :].sort(descending=True)\n",
    "            logits = logits.double()\n",
    "            logits_temp = logits / temp\n",
    "            probs_temp = F.softmax(logits_temp, dim=0)\n",
    "            cum_probs = probs_temp.cumsum(0)\n",
    "\n",
    "            # Cutoff low probabilities that would be rounded to 0\n",
    "            cur_int_range = cur_interval[1] - cur_interval[0]\n",
    "            cur_threshold = 1 / cur_int_range\n",
    "            \n",
    "            k = min(max(2, (probs_temp < cur_threshold).nonzero()[0].item()), topk)\n",
    "            probs_temp_int = probs_temp[:k]  # Cutoff all but top k\n",
    "\n",
    "            # Rescale to correct range\n",
    "            probs_temp_int = probs_temp_int / probs_temp_int.sum() * cur_int_range\n",
    "\n",
    "            # Round probabilities to integers given precision\n",
    "            probs_temp_int = probs_temp_int.round().long()\n",
    "            cum_probs = probs_temp_int.cumsum(0)\n",
    "\n",
    "            # Remove any elements from the bottom if rounding caused the total prob to be too large\n",
    "            overfill_index = (cum_probs > cur_int_range).nonzero()\n",
    "            if len(overfill_index) > 0:\n",
    "                cum_probs = cum_probs[:overfill_index[0]]\n",
    "                k = overfill_index[0].item()\n",
    "\n",
    "            # Add any mass to the top if removing/rounding causes the total prob to be too small\n",
    "            cum_probs += cur_int_range - cum_probs[-1]  # add\n",
    "\n",
    "            # Covnert to position in range\n",
    "            cum_probs += cur_interval[0]\n",
    "\n",
    "            rank = (indices == inp[i]).nonzero().item()\n",
    "\n",
    "            # Handle most errors that could happen because of BPE with heuristic\n",
    "            if rank >= k:\n",
    "                true_token_text = enc.decode(inp[i])\n",
    "                for rank_idx in range(k):\n",
    "                    prop_token_text = enc.decode(indices[rank_idx].item())\n",
    "                    # common case that is not caught\n",
    "                    if inp[i] == 128 and indices[rank_idx] == 198:\n",
    "                        rank = rank_idx\n",
    "                        inp[i] = indices[rank_idx].item()\n",
    "                        break\n",
    "\n",
    "                    # Is there a more likely prefix token that could be the actual token generated?\n",
    "                    if len(prop_token_text) <= len(true_token_text) and \\\n",
    "                            prop_token_text == true_token_text[:len(prop_token_text)]:\n",
    "                        rank = rank_idx\n",
    "                        suffix = true_token_text[len(prop_token_text):]\n",
    "                        suffix_tokens = enc.encode(suffix)  # a list\n",
    "                        inp[i] = indices[rank_idx].item()\n",
    "                        inp[i + 1:i + 1] = suffix_tokens  # insert suffix tokens into list\n",
    "                        break\n",
    "\n",
    "                    # Is there a more likely longer token that could be the actual token generated?\n",
    "                    elif len(prop_token_text) > len(true_token_text) and \\\n",
    "                            true_token_text == prop_token_text[:len(true_token_text)]:\n",
    "                        whole_text = true_token_text\n",
    "                        num_extra = 1\n",
    "                        while len(whole_text) < len(prop_token_text):\n",
    "                            whole_text += enc.decode(inp[i + num_extra])\n",
    "                            num_extra += 1\n",
    "                        if prop_token_text == whole_text[:len(prop_token_text)]:\n",
    "                            rank = rank_idx\n",
    "                            inp[i] = indices[rank_idx].item()\n",
    "                            for j in range(1, num_extra):\n",
    "                                del inp[i + j]\n",
    "\n",
    "                            if len(whole_text) > len(prop_token_text):\n",
    "                                suffix = whole_text[len(prop_token_text):]\n",
    "                                suffix_tokens = enc.encode(suffix)  # a list\n",
    "                                inp[i + 1:i + 1] = suffix_tokens  # insert suffix tokens into list\n",
    "                            break\n",
    "                else:\n",
    "                    #print('Unable to fix BPE error: token received: %s=%d, text: %s' % (true_token_text, inp[i], text))\n",
    "                    rank = 0\n",
    "\n",
    "            selection = rank\n",
    "\n",
    "            # Calculate new range as ints\n",
    "            new_int_bottom = cum_probs[selection - 1] if selection > 0 else cur_interval[0]\n",
    "            new_int_top = cum_probs[selection]\n",
    "\n",
    "            # Convert range to bits\n",
    "            new_int_bottom_bits_inc = list(reversed(int2bits(new_int_bottom, precision)))\n",
    "            new_int_top_bits_inc = list(\n",
    "                reversed(int2bits(new_int_top - 1, precision)))  # -1 here because upper bound is exclusive\n",
    "\n",
    "            # Emit most significant bits which are now fixed and update interval\n",
    "            num_bits_encoded = num_same_from_beg(new_int_bottom_bits_inc, new_int_top_bits_inc)\n",
    "            if i == len(inp) - 1:\n",
    "                new_bits = new_int_bottom_bits_inc\n",
    "            else:\n",
    "                new_bits = new_int_top_bits_inc[:num_bits_encoded]\n",
    "            message += new_bits\n",
    "\n",
    "            #print(\"num bits: \",num_bits_encoded, \"new bot 1: \", new_int_bottom_bits_inc[num_bits_encoded:])\n",
    "            #print(\"new bot2 : \", [1] * num_bits_encoded)\n",
    "            new_int_bottom_bits = new_int_bottom_bits_inc[num_bits_encoded:] + [0] * num_bits_encoded\n",
    "            new_int_top_bits = new_int_top_bits_inc[num_bits_encoded:] + [1] * num_bits_encoded\n",
    "            cur_interval[0] = bits2int(reversed(new_int_bottom_bits))\n",
    "            cur_interval[1] = bits2int(reversed(new_int_top_bits)) + 1  # +1 here because upper bound is exclusive\n",
    "\n",
    "            # Update history with new token\n",
    "            prev = torch.tensor([inp[i]], device=device, dtype=torch.long)\n",
    "            i += 1\n",
    "            \n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code pour le décodage arithmétique (de bits à du texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_arithmetic(model, enc, message, context, finish_sent=False, model_device=\"cuda\", device='cpu', temp=1.0, precision=16,\n",
    "                      topk=50000):\n",
    "    context = torch.tensor(context[-1022:], device=device, dtype=torch.long)\n",
    "\n",
    "    max_val = 2 ** precision\n",
    "    threshold = 2 ** (-precision)\n",
    "    cur_interval = [0, max_val]  # bottom inclusive, top exclusive\n",
    "\n",
    "    prev = context\n",
    "    output = context\n",
    "    past = None\n",
    "\n",
    "    total_num = 0\n",
    "    total_num_for_stats = 0\n",
    "    total_log_probs = 0\n",
    "    total_kl = 0  # in bits\n",
    "    total_entropy_ptau = 0\n",
    "    total_num_sents = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        sent_finish = False\n",
    "        while i < len(message) or (finish_sent and not sent_finish):\n",
    "            outputs = model(prev.unsqueeze(0).to(model_device), past_key_values=past)\n",
    "            logits, past = outputs.logits, outputs.past_key_values\n",
    "            logits = logits.to(device)\n",
    "            \n",
    "            logits[0, -1, -1] = -1e20  # endoftext token can't happen\n",
    "            logits[0, -1, 628] = -1e20  # 2 newlines token can't happen\n",
    "            logits, indices = logits[0, -1, :].sort(descending=True)\n",
    "            logits = logits.double()\n",
    "            logits_temp = logits / temp\n",
    "            probs_temp = F.softmax(logits_temp, dim=0)\n",
    "            log_probs_temp = F.log_softmax(logits_temp, dim=0)\n",
    "            log_probs = F.log_softmax(logits, dim=0)\n",
    "\n",
    "            # conditions for having reached the end of the message\n",
    "            if i >= len(message):\n",
    "                selection = 0\n",
    "                sent_finish = is_sent_finish(indices[selection].item(), enc)\n",
    "            else:\n",
    "                # Cutoff low probabilities that would be rounded to 0\n",
    "                cur_int_range = cur_interval[1] - cur_interval[0]\n",
    "                cur_threshold = 1 / cur_int_range\n",
    "                k = min(max(2, (probs_temp < cur_threshold).nonzero()[0].item()), topk)\n",
    "                probs_temp_int = probs_temp[:k]  # Cutoff all but top k\n",
    "\n",
    "                # Rescale to correct range\n",
    "                probs_temp_int = probs_temp_int / probs_temp_int.sum() * cur_int_range\n",
    "\n",
    "                # Round probabilities to integers given precision\n",
    "                probs_temp_int = probs_temp_int.round().long()\n",
    "                cum_probs = probs_temp_int.cumsum(0)\n",
    "\n",
    "                # Remove any elements from the bottom if rounding caused the total prob to be too large\n",
    "                overfill_index = (cum_probs > cur_int_range).nonzero()\n",
    "                if len(overfill_index) > 0:\n",
    "                    cum_probs = cum_probs[:overfill_index[0]]\n",
    "\n",
    "                # Add any mass to the top if removing/rounding causes the total prob to be too small\n",
    "                cum_probs += cur_int_range - cum_probs[-1]  # add\n",
    "\n",
    "                # Get out resulting probabilities\n",
    "                probs_final = cum_probs.clone()\n",
    "                probs_final[1:] = cum_probs[1:] - cum_probs[:-1]\n",
    "\n",
    "                # Convert to position in range\n",
    "                cum_probs += cur_interval[0]\n",
    "\n",
    "                # Get selected index based on binary fraction from message bits\n",
    "                message_bits = message[i:i + precision]\n",
    "                if i + precision > len(message):\n",
    "                    message_bits = message_bits + [0] * (i + precision - len(message))\n",
    "                message_idx = bits2int(reversed(message_bits))\n",
    "                selection = (cum_probs > message_idx).nonzero()[0].item()\n",
    "\n",
    "                # Calculate new range as ints\n",
    "                new_int_bottom = cum_probs[selection - 1] if selection > 0 else cur_interval[0]\n",
    "                new_int_top = cum_probs[selection]\n",
    "\n",
    "                # Convert range to bits\n",
    "                new_int_bottom_bits_inc = list(reversed(int2bits(new_int_bottom, precision)))\n",
    "                new_int_top_bits_inc = list(\n",
    "                    reversed(int2bits(new_int_top - 1, precision)))  # -1 here because upper bound is exclusive\n",
    "                # Consume most significant bits which are now fixed and update interval\n",
    "                num_bits_encoded = num_same_from_beg(new_int_bottom_bits_inc, new_int_top_bits_inc)\n",
    "                i += num_bits_encoded\n",
    "\n",
    "                new_int_bottom_bits = new_int_bottom_bits_inc[num_bits_encoded:] + [0] * num_bits_encoded\n",
    "                new_int_top_bits = new_int_top_bits_inc[num_bits_encoded:] + [1] * num_bits_encoded\n",
    "\n",
    "                cur_interval[0] = bits2int(reversed(new_int_bottom_bits))\n",
    "                cur_interval[1] = bits2int(reversed(new_int_top_bits)) + 1  # +1 here because upper bound is exclusive\n",
    "\n",
    "\n",
    "            # Update history with new token\n",
    "            prev = indices[selection].view(1)\n",
    "            output = torch.cat((output, prev))\n",
    "            total_num += 1\n",
    "            \n",
    "\n",
    "            # For text->bits->text\n",
    "            partial = enc.decode(output[len(context):].tolist())\n",
    "            \n",
    "            if '<eos>' in partial:\n",
    "                break\n",
    "\n",
    "\n",
    "    return output[len(context):].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder et décoder le message secret pour l'insertion stégo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message secret à encoder: I am hidden\n",
      "\n",
      "message encodé: bitarray('0010110101101100010101000110001110101100111000100100001110111010011110111111010110000110001110000000111000000')\n",
      "\n",
      "message décodé:  I am hidden\n"
     ]
    }
   ],
   "source": [
    "print(\"message secret à encoder: {}\\n\".format(message_secret[:message_secret.index('<eos>')]))\n",
    "bit_message = str2bit(message_secret, tokenizer, model)\n",
    "print(\"message encodé: {}\\n\".format(bit_message))\n",
    "decoded_message = bit2str(bit_message, tokenizer, model)\n",
    "print(\"message décodé: \", decoded_message[:decoded_message.index('<eos>')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stegotexte:  I am going on a vacation to Italy. I am hoping that my new partner will understand this gift.\n",
      "\n",
      "P.S. Please excuse this photo, my boyfriend had to use the camera before the Koyaanisqatsi.\n",
      "\n",
      "Advertisements\n",
      "\n",
      "Share this: Twitter\n",
      "\n",
      "Facebook\n",
      "\n",
      "Google\n",
      "\n",
      "Like this: Like Loading... Related\n",
      "\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "stegotexte = bit2str(bit_message, tokenizer, model, context=amorce)\n",
    "print(\"stegotexte: \", amorce+stegotexte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits encodés par le texte stego: bitarray('00101101011011000101010001100011101011001110001001000011101110100111101111110101100001100011100000001110000000000000000000000000000000000000011111111111111111111010001110101001111001110000010111011000110111011101011001001100000000000000000000') \n",
      "\n",
      "Message décodé:  I am hidden\n"
     ]
    }
   ],
   "source": [
    "bit_message_new = str2bit(stegotexte, tokenizer, model, context=amorce)\n",
    "print(\"Bits encodés par le texte stego: {} \\n\".format(bit_message_new))\n",
    "decoded_message = bit2str(bit_message_new, tokenizer, model)\n",
    "print(\"Message décodé: \", decoded_message[:decoded_message.index('<eos>')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
