{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import bitarray\n",
    "#from pytorch_transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#from transformers import CamembertLMHeadModel, CamembertTokenizer, CamembertForCausalLM, AutoTokenizer\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_secret = \"I dont want to be too cocky but here is a long ass message\"\n",
    "#message_secret = message_secret + '<eos>'\n",
    "\n",
    "amorce = \"I am going on a vacation to Italy. I am hoping that my\"\n",
    "\n",
    "model_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_past(past):\n",
    "    past = list(past)\n",
    "    for i in range(len(past)):\n",
    "        past[i] = past[i][:, :, :, -1022:]\n",
    "    return past\n",
    "\n",
    "def int2bits(inp, num_bits):\n",
    "    if num_bits == 0:\n",
    "        return []\n",
    "    strlist = ('{0:0%db}' % num_bits).format(inp)\n",
    "    return [int(strval) for strval in reversed(strlist)]\n",
    "\n",
    "def num_same_from_beg(bits1, bits2):\n",
    "    assert len(bits1) == len(bits2)\n",
    "    for i in range(len(bits1)):\n",
    "        if bits1[i] != bits2[i]:\n",
    "            break\n",
    "\n",
    "    return i\n",
    "\n",
    "def bits2int(bits):\n",
    "    res = 0\n",
    "    for i, bit in enumerate(bits):\n",
    "        res += bit * (2 ** i)\n",
    "    return res\n",
    "\n",
    "\n",
    "def str2bit(msg_str, tokenizer, model, context=None):\n",
    "    if context is None:\n",
    "        message_ctx = tokenizer.encode('<|endoftext|>')\n",
    "    else: message_ctx = tokenizer.encode(context)\n",
    "    #msg_str = \"I am hidden\"\n",
    "    msg_str += '<eos>'\n",
    "    msg_bits = bitarray.bitarray()\n",
    "    msg_enc = decode_arithmetic(model, tokenizer, msg_str, message_ctx,\n",
    "                                    precision=40, topk=60000, device='cpu')\n",
    "    msg_bits = bitarray.bitarray(msg_enc)\n",
    "        \n",
    "    return msg_bits\n",
    "\n",
    "def bit2str(msg_bits, tokenizer, model, context=None):\n",
    "    if context is None:\n",
    "        message_ctx = tokenizer.encode('<|endoftext|>')\n",
    "    else: \n",
    "        message_ctx = tokenizer.encode(context)\n",
    "        \n",
    "    msg_str = encode_arithmetic(model, tokenizer, msg_bits, message_ctx,\n",
    "        precision=40, topk=60000, device=\"cpu\", model_device='cpu')\n",
    "    print(\"msg str: \", msg_str, msg_str)\n",
    "    msg_str = tokenizer.decode(msg_str)\n",
    "    return msg_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#tokenizer = MeteorTokenizer.from_pretrained(model_name)\n",
    "#tokenizer.unk_token = None\n",
    "#tokenizer.bos_token = None\n",
    "#tokenizer.eos_token = None\n",
    "\n",
    "#model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going on a vacation to Italy. I am hoping that my\n",
      "[40, 716, 1016, 319, 257, 14600, 284, 8031, 13, 314, 716, 7725, 326, 616]\n",
      "I dont want to be too cocky but here is a long ass message\n",
      "[40, 17666, 765, 284, 307, 1165, 7540, 88, 475, 994, 318, 257, 890, 840, 3275]\n"
     ]
    }
   ],
   "source": [
    "encoded_message = tokenizer.encode(message_secret)\n",
    "encoded_context = tokenizer.encode(amorce)\n",
    "\n",
    "print(amorce)\n",
    "print(encoded_context)\n",
    "\n",
    "print(message_secret)\n",
    "print(encoded_message)\n",
    "\n",
    "tensor_amorce = torch.LongTensor(encoded_context).view(1,-1)\n",
    "tensor_message = torch.LongTensor(encoded_message).view(1,-1)\n",
    "#liste des tokens visibles sur:\n",
    "#https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 716, 1016, 319, 257, 14600, 284, 8031, 13, 314, 716, 7725, 326, 616, 3656, 481, 307, 1498, 284, 1282, 736, 2582, 13, 314, 716, 1016, 284, 307, 736]\n",
      "I am going on a vacation to Italy. I am hoping that my wife will be able to come back soon. I am going to be back\n"
     ]
    }
   ],
   "source": [
    "tokens_temp = encoded_context.copy()\n",
    "tensor_tokens = torch.LongTensor(tokens_temp).view(1,-1)\n",
    "for i in range(15):\n",
    "    with torch.no_grad():\n",
    "            outputs = model(tensor_tokens)\n",
    "            logits, past = outputs.logits, outputs.past_key_values\n",
    "            \n",
    "            tokens_temp.append(logits[0,-1].argsort()[-1].item())\n",
    "            tensor_tokens = torch.LongTensor(tokens_temp).view(1,-1)\n",
    "print(tokens_temp)\n",
    "print(tokenizer.decode(tokens_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 716, 1016, 319, 257, 14600, 284, 8031, 13, 314, 716, 7725, 326, 616, 1641, 11, 508, 389, 523, 1611, 11, 290, 284, 423, 884, 281, 6275, 9912, 13, 383, 1266, 6979, 373, 281]\n",
      "I am going on a vacation to Italy. I am hoping that my family, who are so kind, and to have such an excellent holiday. The best gift was an\n"
     ]
    }
   ],
   "source": [
    "tokens_temp = encoded_context.copy()\n",
    "tensor_tokens = torch.LongTensor(tokens_temp).view(1,-1)\n",
    "\n",
    "for i in range(20):\n",
    "    with torch.no_grad():\n",
    "            outputs = model(tensor_tokens)\n",
    "            logits, past = outputs.logits, outputs.past_key_values\n",
    "            \n",
    "            tokens_temp.append(logits[0,-1].argsort()[-round(random.random()*10)-1].item())\n",
    "            tensor_tokens = torch.LongTensor(tokens_temp).view(1,-1)\n",
    "print(tokens_temp)\n",
    "print(tokenizer.decode(tokens_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I dont want to be too cocky but here is a long ass message\n",
      "tensor([50256])\n",
      "9.094947017729282e-13\n",
      "[0, 0, 1, 0]\n",
      "tensor([40])\n",
      "3.0938210371382067e-12\n",
      "[1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]\n",
      "tensor([17666])\n",
      "2.3179197916547297e-12\n",
      "[1, 0, 0, 0]\n",
      "tensor([765])\n",
      "2.890773375613963e-12\n",
      "[]\n",
      "tensor([284])\n",
      "3.954451855135105e-12\n",
      "[0, 1]\n",
      "tensor([307])\n",
      "1.0748407960903596e-11\n",
      "[0, 1, 1, 1, 0, 0, 0, 0, 1]\n",
      "tensor([1165])\n",
      "1.5807423970294914e-12\n",
      "[1, 1, 0, 0, 0, 0, 0, 1, 0]\n",
      "tensor([7540])\n",
      "1.4325669280030835e-12\n",
      "[]\n",
      "tensor([88])\n",
      "1.4530086583045491e-12\n",
      "[0, 1]\n",
      "tensor([475])\n",
      "5.54258535083705e-12\n",
      "[0, 1, 1, 1, 0, 1, 1]\n",
      "tensor([994])\n",
      "3.15972850394505e-12\n",
      "[0]\n",
      "tensor([318])\n",
      "3.838655971446136e-12\n",
      "[]\n",
      "tensor([257])\n",
      "1.5158692518647556e-11\n",
      "[1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1]\n",
      "tensor([890])\n",
      "1.7334601282257352e-12\n",
      "[1, 0, 0, 1, 0, 1, 0, 0]\n",
      "tensor([840])\n",
      "6.06391349454872e-12\n",
      "[0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1]\n",
      "tensor([3275])\n",
      "1.9324447033835973e-12\n",
      "[1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "tensor([27])\n",
      "1.7725481207756337e-12\n",
      "[1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1]\n",
      "tensor([68])\n",
      "5.1507425919709215e-12\n",
      "[1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1]\n",
      "tensor([418])\n",
      "1.84964614679404e-12\n",
      "[0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "encoded message:  bitarray('00101001000100101000010111000011100000100101110110100001100101100101000110111100111011111101111000011010011001110100010111101010001110000010100001010010000000000000000')\n"
     ]
    }
   ],
   "source": [
    "def decode_arithmetic(model, enc, text, context, device='cuda', temp=1.0, precision=16, topk=50000):\n",
    "    # inp is a list of token indices\n",
    "    # context is a list of token indices\n",
    "    inp = enc.encode(text)\n",
    "    # common BPE error case: 128, 128 (2 newlines) is interpretted as 628 (2 newlines)\n",
    "    i = 0\n",
    "    while i < len(inp):\n",
    "        if inp[i] == 628:\n",
    "            inp[i] = 198\n",
    "            inp[i + 1:i + 1] = [198]\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    context = torch.tensor(context[-1022:], device=device, dtype=torch.long)\n",
    "\n",
    "    max_val = 2 ** precision\n",
    "    threshold = 2 ** (-precision)\n",
    "    cur_interval = [0, max_val]  # bottom inclusive, top exclusive\n",
    "\n",
    "    prev = context\n",
    "    past = None\n",
    "    message = []\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        while i < len(inp):\n",
    "            print(prev)\n",
    "            outputs = model(prev.unsqueeze(0), past_key_values=past)\n",
    "            past, logits = outputs.past_key_values, outputs.logits\n",
    "            #if past is not None:\n",
    "            #    past = limit_past(past)\n",
    "            logits[0, -1, -1] = -1e10  # endoftext can't happen\n",
    "            logits[0, -1, 628] = -1e10  # 2 newlines can't happen\n",
    "            logits, indices = logits[0, -1, :].sort(descending=True)\n",
    "            logits = logits.double()\n",
    "            logits_temp = logits / temp\n",
    "            probs_temp = F.softmax(logits_temp, dim=0)\n",
    "            cum_probs = probs_temp.cumsum(0)\n",
    "\n",
    "            # Cutoff low probabilities that would be rounded to 0\n",
    "            cur_int_range = cur_interval[1] - cur_interval[0]\n",
    "            cur_threshold = 1 / cur_int_range\n",
    "            print(cur_threshold)\n",
    "            \n",
    "            k = min(max(2, (probs_temp < cur_threshold).nonzero()[0].item()), topk)\n",
    "            probs_temp_int = probs_temp[:k]  # Cutoff all but top k\n",
    "\n",
    "            # Rescale to correct range\n",
    "            probs_temp_int = probs_temp_int / probs_temp_int.sum() * cur_int_range\n",
    "\n",
    "            # Round probabilities to integers given precision\n",
    "            probs_temp_int = probs_temp_int.round().long()\n",
    "            cum_probs = probs_temp_int.cumsum(0)\n",
    "\n",
    "            # Remove any elements from the bottom if rounding caused the total prob to be too large\n",
    "            overfill_index = (cum_probs > cur_int_range).nonzero()\n",
    "            if len(overfill_index) > 0:\n",
    "                cum_probs = cum_probs[:overfill_index[0]]\n",
    "                k = overfill_index[0].item()\n",
    "\n",
    "            # Add any mass to the top if removing/rounding causes the total prob to be too small\n",
    "            cum_probs += cur_int_range - cum_probs[-1]  # add\n",
    "\n",
    "            # Covnert to position in range\n",
    "            cum_probs += cur_interval[0]\n",
    "\n",
    "            rank = (indices == inp[i]).nonzero().item()\n",
    "\n",
    "            # Handle most errors that could happen because of BPE with heuristic\n",
    "            if rank >= k:\n",
    "                true_token_text = enc.decoder[inp[i]]\n",
    "                for rank_idx in range(k):\n",
    "                    prop_token_text = enc.decoder[indices[rank_idx].item()]\n",
    "                    # common case that is not caught\n",
    "                    if inp[i] == 128 and indices[rank_idx] == 198:\n",
    "                        rank = rank_idx\n",
    "                        inp[i] = indices[rank_idx].item()\n",
    "                        break\n",
    "\n",
    "                    # Is there a more likely prefix token that could be the actual token generated?\n",
    "                    if len(prop_token_text) <= len(true_token_text) and \\\n",
    "                            prop_token_text == true_token_text[:len(prop_token_text)]:\n",
    "                        rank = rank_idx\n",
    "                        suffix = true_token_text[len(prop_token_text):]\n",
    "                        suffix_tokens = enc.encode(suffix)  # a list\n",
    "                        inp[i] = indices[rank_idx].item()\n",
    "                        inp[i + 1:i + 1] = suffix_tokens  # insert suffix tokens into list\n",
    "                        break\n",
    "\n",
    "                    # Is there a more likely longer token that could be the actual token generated?\n",
    "                    elif len(prop_token_text) > len(true_token_text) and \\\n",
    "                            true_token_text == prop_token_text[:len(true_token_text)]:\n",
    "                        whole_text = true_token_text\n",
    "                        num_extra = 1\n",
    "                        while len(whole_text) < len(prop_token_text):\n",
    "                            whole_text += enc.decoder[inp[i + num_extra]]\n",
    "                            num_extra += 1\n",
    "                        if prop_token_text == whole_text[:len(prop_token_text)]:\n",
    "                            rank = rank_idx\n",
    "                            inp[i] = indices[rank_idx].item()\n",
    "                            for j in range(1, num_extra):\n",
    "                                del inp[i + j]\n",
    "\n",
    "                            if len(whole_text) > len(prop_token_text):\n",
    "                                suffix = whole_text[len(prop_token_text):]\n",
    "                                suffix_tokens = enc.encode(suffix)  # a list\n",
    "                                inp[i + 1:i + 1] = suffix_tokens  # insert suffix tokens into list\n",
    "                            break\n",
    "                else:\n",
    "                    print('Unable to fix BPE error: token received: %s=%d, text: %s' % (true_token_text, inp[i], text))\n",
    "                    rank = 0\n",
    "\n",
    "            selection = rank\n",
    "\n",
    "            # Calculate new range as ints\n",
    "            new_int_bottom = cum_probs[selection - 1] if selection > 0 else cur_interval[0]\n",
    "            new_int_top = cum_probs[selection]\n",
    "\n",
    "            # Convert range to bits\n",
    "            new_int_bottom_bits_inc = list(reversed(int2bits(new_int_bottom, precision)))\n",
    "            new_int_top_bits_inc = list(\n",
    "                reversed(int2bits(new_int_top - 1, precision)))  # -1 here because upper bound is exclusive\n",
    "\n",
    "            # Emit most significant bits which are now fixed and update interval\n",
    "            num_bits_encoded = num_same_from_beg(new_int_bottom_bits_inc, new_int_top_bits_inc)\n",
    "            if i == len(inp) - 1:\n",
    "                new_bits = new_int_bottom_bits_inc\n",
    "            else:\n",
    "                new_bits = new_int_top_bits_inc[:num_bits_encoded]\n",
    "            message += new_bits\n",
    "            print(new_bits)\n",
    "\n",
    "            #print(\"num bits: \",num_bits_encoded, \"new bot 1: \", new_int_bottom_bits_inc[num_bits_encoded:])\n",
    "            #print(\"new bot2 : \", [1] * num_bits_encoded)\n",
    "            new_int_bottom_bits = new_int_bottom_bits_inc[num_bits_encoded:] + [0] * num_bits_encoded\n",
    "            new_int_top_bits = new_int_top_bits_inc[num_bits_encoded:] + [1] * num_bits_encoded\n",
    "            cur_interval[0] = bits2int(reversed(new_int_bottom_bits))\n",
    "            cur_interval[1] = bits2int(reversed(new_int_top_bits)) + 1  # +1 here because upper bound is exclusive\n",
    "\n",
    "            # Update history with new token\n",
    "            prev = torch.tensor([inp[i]], device=device, dtype=torch.long)\n",
    "            i += 1\n",
    "            \n",
    "    return message\n",
    "\n",
    "print(message_secret)\n",
    "bit_message = str2bit(message_secret, tokenizer, model)\n",
    "print(\"encoded message: \", bit_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah = np.array([[0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "eh = [0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167 108\n"
     ]
    }
   ],
   "source": [
    "print(len(bit_message), len(eh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I dont want to be too cocky but here is a long ass message\n",
      "z est reparti :  tensor([0.0625, 0.1003, 0.1245, 0.1439, 0.1623, 0.1739, 0.1852, 0.1944, 0.2031,\n",
      "        0.2109], dtype=torch.float64) 0 1099511627776\n",
      "tensor([ 198,  464,    1,   32,   40,  818,   13, 1026,   50, 1212])\n",
      "==========================================\n",
      "==========================================\n",
      "4 I\n",
      "z est reparti :  tensor([0.0934, 0.1815, 0.2446, 0.2920, 0.3389, 0.3677, 0.3938, 0.4166, 0.4380,\n",
      "        0.4543], dtype=torch.float64) 332922693888 656147595392\n",
      "tensor([1101, 1053,  423,  716,  373,  836,  892,  550,  760,  460])\n",
      "==========================================\n",
      "==========================================\n",
      "217 I dont\n",
      "z est reparti :  tensor([0.3777, 0.5191, 0.5724, 0.6225, 0.6582, 0.6863, 0.7138, 0.7377, 0.7612,\n",
      "        0.7787], dtype=torch.float64) 327169826816 758591139840\n",
      "tensor([ 760,  892,  423,  765, 1107, 3505,  588, 1833,  766,  772])\n",
      "==========================================\n",
      "==========================================\n",
      "3 I dont want\n",
      "z est reparti :  tensor([0.7310, 0.7663, 0.7898, 0.8074, 0.8225, 0.8354, 0.8479, 0.8589, 0.8697,\n",
      "        0.8787], dtype=torch.float64) 389766355392 735694544864\n",
      "tensor([ 284,  345,  428,  262,  257,  616,  340,  661,  597, 2687])\n",
      "==========================================\n",
      "==========================================\n",
      "0 I dont want to\n",
      "z est reparti :  tensor([0.0920, 0.1532, 0.2067, 0.2387, 0.2687, 0.2950, 0.3173, 0.3367, 0.3558,\n",
      "        0.3746], dtype=torch.float64) 389766355392 642645903970\n",
      "tensor([ 307,  651,  910, 1561,  467,  787, 1577,  766,  466, 3551])\n",
      "==========================================\n",
      "==========================================\n",
      "0 I dont want to be\n",
      "z est reparti :  tensor([0.1213, 0.1805, 0.2059, 0.2269, 0.2456, 0.2637, 0.2769, 0.2895, 0.3002,\n",
      "        0.3109], dtype=torch.float64) 459553793792 552590828080\n",
      "tensor([  257,   262,   287,   281,   588, 22066,  1165,  1444,  5371,  1498])\n",
      "==========================================\n",
      "==========================================\n",
      "6 I dont want to be too\n",
      "z est reparti :  tensor([0.0955, 0.1598, 0.1858, 0.2044, 0.2210, 0.2374, 0.2530, 0.2685, 0.2827,\n",
      "        0.2944], dtype=torch.float64) 460811409920 1093425554944\n",
      "tensor([11859,  2176,  1327,   881,  4688,  6276,  2298,  4633, 19861,   890])\n",
      "==========================================\n",
      "==========================================\n",
      "77 I dont want to be too cock\n",
      "z est reparti :  tensor([0.9859, 0.9878, 0.9894, 0.9901, 0.9907, 0.9912, 0.9916, 0.9920, 0.9924,\n",
      "        0.9927], dtype=torch.float64) 101408954880 799456620544\n",
      "tensor([  88,   12, 1837,  813,  265, 1681,  397,   83,  959,  680])\n",
      "==========================================\n",
      "==========================================\n",
      "0 I dont want to be too cocky\n",
      "z est reparti :  tensor([0.1509, 0.2750, 0.3983, 0.4638, 0.5237, 0.5720, 0.6142, 0.6550, 0.6772,\n",
      "        0.6964], dtype=torch.float64) 101408954880 789636099491\n",
      "tensor([ 11,  13, 546, 475, 351, 994, 290, 393, 618, 287])\n",
      "==========================================\n",
      "==========================================\n",
      "3 I dont want to be too cocky but\n",
      "z est reparti :  tensor([0.2726, 0.3561, 0.4064, 0.4534, 0.4862, 0.5104, 0.5329, 0.5527, 0.5712,\n",
      "        0.5894], dtype=torch.float64) 402568837692 582990055716\n",
      "tensor([ 314, 1312,  340,  428,  611,  262,  345,  618,  616,  612])\n",
      "==========================================\n",
      "==========================================\n",
      "10 I dont want to be too cocky but here\n",
      "z est reparti :  tensor([0.4116, 0.6169, 0.7748, 0.8108, 0.8425, 0.8707, 0.8939, 0.9130, 0.9250,\n",
      "        0.9348], dtype=torch.float64) 268647855360 585130742656\n",
      "tensor([ 318,  338,  389,  340, 2925,  356,  314,  345, 1312, 2058])\n",
      "==========================================\n",
      "==========================================\n",
      "0 I dont want to be too cocky but here is\n",
      "z est reparti :  tensor([0.2532, 0.4023, 0.5491, 0.6513, 0.7016, 0.7508, 0.7883, 0.8214, 0.8445,\n",
      "        0.8638], dtype=torch.float64) 537295710720 797803557070\n",
      "tensor([ 257,  616,  262,  644,  703,  617,  281, 1223,  530,  534])\n",
      "==========================================\n",
      "==========================================\n",
      "0 I dont want to be too cocky but here is a\n",
      "z est reparti :  tensor([0.0826, 0.1366, 0.1830, 0.2212, 0.2580, 0.2864, 0.3074, 0.3279, 0.3455,\n",
      "        0.3622], dtype=torch.float64) 537295710720 603264460912\n",
      "tensor([4286, 1351, 2008, 2068, 2792, 1310,  922, 1790, 3621, 2829])\n",
      "==========================================\n",
      "==========================================\n",
      "56 I dont want to be too cocky but here is a long\n",
      "z est reparti :  tensor([0.1480, 0.2333, 0.3036, 0.3633, 0.3941, 0.4205, 0.4462, 0.4687, 0.4884,\n",
      "        0.5035], dtype=torch.float64) 205566296064 782447173632\n",
      "tensor([ 1351,  1621,  1281,  2008,  9577,   290, 30993,  2708,   530,  6764])\n",
      "==========================================\n",
      "==========================================\n",
      "81 I dont want to be too cocky but here is a long ass\n",
      "z est reparti :  tensor([0.1083, 0.1811, 0.2029, 0.2195, 0.2354, 0.2496, 0.2605, 0.2702, 0.2795,\n",
      "        0.2880], dtype=torch.float64) 392124037888 557034042112\n",
      "tensor([ 2008,  4286,  4590,  1281,  1351,  1621,  8301,  2823, 30993,  3704])\n",
      "==========================================\n",
      "==========================================\n",
      "86 I dont want to be too cocky but here is a long ass message\n",
      "z est reparti :  tensor([0.1794, 0.3547, 0.4892, 0.5489, 0.6037, 0.6582, 0.6886, 0.7099, 0.7225,\n",
      "        0.7340], dtype=torch.float64) 448981200896 966460432384\n",
      "tensor([422, 329, 284, 326, 314,  13, 546,  25,  11, 351])\n",
      "==========================================\n",
      "==========================================\n",
      "2333 I dont want to be too cocky but here is a long ass message<\n",
      "z est reparti :  tensor([0.2777, 0.3422, 0.3734, 0.3927, 0.4084, 0.4209, 0.4326, 0.4413, 0.4497,\n",
      "        0.4572], dtype=torch.float64) 412391833600 976551411712\n",
      "tensor([  18, 1671,   29,  198,  438,   25,   19,  357, 4023,   17])\n",
      "==========================================\n",
      "==========================================\n",
      "779 I dont want to be too cocky but here is a long ass message<e\n",
      "z est reparti :  tensor([0.0629, 0.1241, 0.1789, 0.2291, 0.2707, 0.3070, 0.3281, 0.3477, 0.3631,\n",
      "        0.3749], dtype=torch.float64) 521274843136 715421605888\n",
      "tensor([ 538,    8,   60,   29,   13,   12,  988, 2768,   25,   18])\n",
      "==========================================\n",
      "==========================================\n",
      "1277 I dont want to be too cocky but here is a long ass message<eos\n",
      "z est reparti :  tensor([0.4566, 0.4882, 0.5156, 0.5371, 0.5579, 0.5715, 0.5832, 0.5938, 0.6038,\n",
      "        0.6128], dtype=torch.float64) 120597315584 661241266176\n",
      "tensor([   29, 28401,    13,    11,    25,    12,  6927, 31175, 22330,    62])\n",
      "==========================================\n",
      "==========================================\n",
      "msg str:  [40, 17666, 765, 284, 307, 1165, 7540, 88, 475, 994, 318, 257, 890, 840, 3275, 27, 68, 418, 29] [40, 17666, 765, 284, 307, 1165, 7540, 88, 475, 994, 318, 257, 890, 840, 3275, 27, 68, 418, 29]\n",
      "decoded message:  I dont want to be too cocky but here is a long ass message<eos>\n"
     ]
    }
   ],
   "source": [
    "def encode_arithmetic(model, enc, message, context, finish_sent=False, model_device=\"cuda\", device='cpu', temp=1.0, precision=16,\n",
    "                      topk=50000):\n",
    "    context = torch.tensor(context[-1022:], device=device, dtype=torch.long)\n",
    "\n",
    "    max_val = 2 ** precision\n",
    "    threshold = 2 ** (-precision)\n",
    "    cur_interval = [0, max_val]  # bottom inclusive, top exclusive\n",
    "\n",
    "    prev = context\n",
    "    output = context\n",
    "    past = None\n",
    "\n",
    "    total_num = 0\n",
    "    total_num_for_stats = 0\n",
    "    total_log_probs = 0\n",
    "    total_kl = 0  # in bits\n",
    "    total_entropy_ptau = 0\n",
    "    total_num_sents = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        sent_finish = False\n",
    "        while i < len(message) or (finish_sent and not sent_finish):\n",
    "            outputs = model(prev.unsqueeze(0).to(model_device), past_key_values=past)\n",
    "            logits, past = outputs.logits, outputs.past_key_values\n",
    "            logits = logits.to(device)\n",
    "            \n",
    "            logits[0, -1, -1] = -1e20  # endoftext token can't happen\n",
    "            logits[0, -1, 628] = -1e20  # 2 newlines token can't happen\n",
    "            logits, indices = logits[0, -1, :].sort(descending=True)\n",
    "            logits = logits.double()\n",
    "            logits_temp = logits / temp\n",
    "            probs_temp = F.softmax(logits_temp, dim=0)\n",
    "            log_probs_temp = F.log_softmax(logits_temp, dim=0)\n",
    "            log_probs = F.log_softmax(logits, dim=0)\n",
    "            print('z est reparti : ', probs_temp.cumsum(0)[:10], cur_interval[0], cur_interval[1])\n",
    "\n",
    "            # conditions for having reached the end of the message\n",
    "            if i >= len(message):\n",
    "                selection = 0\n",
    "                sent_finish = is_sent_finish(indices[selection].item(), enc)\n",
    "            else:\n",
    "                # Cutoff low probabilities that would be rounded to 0\n",
    "                cur_int_range = cur_interval[1] - cur_interval[0]\n",
    "                cur_threshold = 1 / cur_int_range\n",
    "                k = min(max(2, (probs_temp < cur_threshold).nonzero()[0].item()), topk)\n",
    "                probs_temp_int = probs_temp[:k]  # Cutoff all but top k\n",
    "\n",
    "                # Rescale to correct range\n",
    "                probs_temp_int = probs_temp_int / probs_temp_int.sum() * cur_int_range\n",
    "\n",
    "                # Round probabilities to integers given precision\n",
    "                probs_temp_int = probs_temp_int.round().long()\n",
    "                cum_probs = probs_temp_int.cumsum(0)\n",
    "\n",
    "                # Remove any elements from the bottom if rounding caused the total prob to be too large\n",
    "                overfill_index = (cum_probs > cur_int_range).nonzero()\n",
    "                if len(overfill_index) > 0:\n",
    "                    cum_probs = cum_probs[:overfill_index[0]]\n",
    "\n",
    "                # Add any mass to the top if removing/rounding causes the total prob to be too small\n",
    "                cum_probs += cur_int_range - cum_probs[-1]  # add\n",
    "\n",
    "                # Get out resulting probabilities\n",
    "                probs_final = cum_probs.clone()\n",
    "                probs_final[1:] = cum_probs[1:] - cum_probs[:-1]\n",
    "\n",
    "                # Convert to position in range\n",
    "                cum_probs += cur_interval[0]\n",
    "\n",
    "                # Get selected index based on binary fraction from message bits\n",
    "                message_bits = message[i:i + precision]\n",
    "                if i + precision > len(message):\n",
    "                    message_bits = message_bits + [0] * (i + precision - len(message))\n",
    "                message_idx = bits2int(reversed(message_bits))\n",
    "                selection = (cum_probs > message_idx).nonzero()[0].item()\n",
    "\n",
    "                # Calculate new range as ints\n",
    "                new_int_bottom = cum_probs[selection - 1] if selection > 0 else cur_interval[0]\n",
    "                new_int_top = cum_probs[selection]\n",
    "\n",
    "                # Convert range to bits\n",
    "                new_int_bottom_bits_inc = list(reversed(int2bits(new_int_bottom, precision)))\n",
    "                new_int_top_bits_inc = list(\n",
    "                    reversed(int2bits(new_int_top - 1, precision)))  # -1 here because upper bound is exclusive\n",
    "                # Consume most significant bits which are now fixed and update interval\n",
    "                num_bits_encoded = num_same_from_beg(new_int_bottom_bits_inc, new_int_top_bits_inc)\n",
    "                i += num_bits_encoded\n",
    "\n",
    "                new_int_bottom_bits = new_int_bottom_bits_inc[num_bits_encoded:] + [0] * num_bits_encoded\n",
    "                new_int_top_bits = new_int_top_bits_inc[num_bits_encoded:] + [1] * num_bits_encoded\n",
    "\n",
    "                cur_interval[0] = bits2int(reversed(new_int_bottom_bits))\n",
    "                cur_interval[1] = bits2int(reversed(new_int_top_bits)) + 1  # +1 here because upper bound is exclusive\n",
    "\n",
    "\n",
    "            # Update history with new token\n",
    "            prev = indices[selection].view(1)\n",
    "            output = torch.cat((output, prev))\n",
    "            total_num += 1\n",
    "            \n",
    "            print(indices[:10])\n",
    "\n",
    "            # For text->bits->text\n",
    "            partial = enc.decode(output[len(context):].tolist())\n",
    "            print(\"==========================================\")\n",
    "            print(\"==========================================\")\n",
    "            if '<eos>' in partial:\n",
    "                break\n",
    "\n",
    "            print(selection, partial)\n",
    "\n",
    "    return output[len(context):].tolist()\n",
    "\n",
    "\n",
    "\n",
    "print(message_secret)\n",
    "decoded_message = bit2str(bit_message, tokenizer, model)\n",
    "print(\"decoded message: \", decoded_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am hidden\n",
      "z est reparti :  tensor([0.0824, 0.1287, 0.1568, 0.1798, 0.2009, 0.2190, 0.2364, 0.2536, 0.2696,\n",
      "        0.2825], dtype=torch.float64) 0 1099511627776\n",
      "tensor([3656, 1641, 5229, 3367, 1545, 5296, 3397, 2460, 4957, 2988])\n",
      "==========================================\n",
      "==========================================\n",
      "2  husband\n",
      "z est reparti :  tensor([0.4606, 0.6631, 0.7136, 0.7574, 0.7925, 0.8078, 0.8184, 0.8289, 0.8377,\n",
      "        0.8464], dtype=torch.float64) 64530923072 559138766160\n",
      "tensor([ 481,  290,  460,   11,  318,  338,  743, 1839,  393,  468])\n",
      "==========================================\n",
      "==========================================\n",
      "3  husband,\n",
      "z est reparti :  tensor([0.1912, 0.2677, 0.3014, 0.3308, 0.3562, 0.3734, 0.3902, 0.4028, 0.4119,\n",
      "        0.4207], dtype=torch.float64) 165110590752 859182163360\n",
      "tensor([ 508,  616,  262,  290,  257,  314,  465, 2988, 4150,  355])\n",
      "==========================================\n",
      "==========================================\n",
      "12  husband, wife\n",
      "z est reparti :  tensor([0.4839, 0.9493, 0.9667, 0.9696, 0.9725, 0.9743, 0.9759, 0.9771, 0.9782,\n",
      "        0.9790], dtype=torch.float64) 311723858368 683261009472\n",
      "tensor([ 290,   11,  393, 1222,   12,  357,  481,   14, 3503,  286])\n",
      "==========================================\n",
      "==========================================\n",
      "1  husband, wife,\n",
      "z est reparti :  tensor([0.5675, 0.6256, 0.6718, 0.7092, 0.7348, 0.7504, 0.7627, 0.7728, 0.7827,\n",
      "        0.7919], dtype=torch.float64) 491519689368 664415118453\n",
      "tensor([  290,  4957,  1751,  3367,  3988,  1641,   393, 14850,   734,  1200])\n",
      "==========================================\n",
      "==========================================\n",
      "65  husband, wife, our\n",
      "z est reparti :  tensor([0.1726, 0.2704, 0.3659, 0.4241, 0.4704, 0.5095, 0.5463, 0.5758, 0.5965,\n",
      "        0.6167], dtype=torch.float64) 430524628992 883587829760\n",
      "tensor([ 1751,  4957,   734,  1115,  3367,  1641,  3988,  4950,  1440, 14850])\n",
      "==========================================\n",
      "==========================================\n",
      "1  husband, wife, our daughter\n",
      "z est reparti :  tensor([0.4488, 0.8087, 0.8643, 0.8800, 0.8884, 0.8955, 0.9003, 0.9047, 0.9084,\n",
      "        0.9119], dtype=torch.float64) 508720636421 553020486139\n",
      "tensor([ 11, 290, 481,  12, 393, 357, 338, 460, 318, 743])\n",
      "==========================================\n",
      "==========================================\n",
      "5  husband, wife, our daughter (\n",
      "z est reparti :  tensor([0.1287, 0.2416, 0.2788, 0.3141, 0.3338, 0.3491, 0.3630, 0.3760, 0.3879,\n",
      "        0.3994], dtype=torch.float64) 480278266880 800528136192\n",
      "tensor([ 8727,   392,  1820, 29642,  7091,  1169,    64,  4480,   372,    40])\n",
      "==========================================\n",
      "==========================================\n",
      "61  husband, wife, our daughter (you\n",
      "z est reparti :  tensor([0.2458, 0.3582, 0.4444, 0.5260, 0.5622, 0.5910, 0.6153, 0.6364, 0.6571,\n",
      "        0.6776], dtype=torch.float64) 314571387392 690639790080\n",
      "tensor([ 760,  389,  481,  460,  821,  423,  743,  290, 1183,    8])\n",
      "==========================================\n",
      "==========================================\n",
      "26  husband, wife, our daughter (you won\n",
      "z est reparti :  tensor([0.9961, 0.9965, 0.9969, 0.9972, 0.9975, 0.9977, 0.9978, 0.9979, 0.9980,\n",
      "        0.9981], dtype=torch.float64) 540354678880 555814232088\n",
      "tensor([  470,     8,   828, 18265,    11,   262,     6,    13,  8133,   257])\n",
      "==========================================\n",
      "==========================================\n",
      "0  husband, wife, our daughter (you won't\n",
      "z est reparti :  tensor([0.4122, 0.5129, 0.6022, 0.6664, 0.7112, 0.7543, 0.7787, 0.8027, 0.8173,\n",
      "        0.8282], dtype=torch.float64) 540354678880 555753498726\n",
      "tensor([1975,  766,  307, 1064,  760, 3285,  423,  765,  651,  892])\n",
      "==========================================\n",
      "==========================================\n",
      "18  husband, wife, our daughter (you won't like\n",
      "z est reparti :  tensor([0.1872, 0.3712, 0.5091, 0.6238, 0.6835, 0.7284, 0.7723, 0.8076, 0.8331,\n",
      "        0.8570], dtype=torch.float64) 218379583488 632089702400\n",
      "tensor([607, 340, 502, 428, 326, 262, 616, 683, 514, 606])\n",
      "==========================================\n",
      "==========================================\n",
      "0  husband, wife, our daughter (you won't like her\n",
      "z est reparti :  tensor([0.1638, 0.2976, 0.4249, 0.4725, 0.4990, 0.5210, 0.5412, 0.5602, 0.5792,\n",
      "        0.5970], dtype=torch.float64) 436759166976 591611097742\n",
      "tensor([   8,  828,   11,  379, 1165,  780,  475,  881, 8133,  326])\n",
      "==========================================\n",
      "==========================================\n",
      "22  husband, wife, our daughter (you won't like her,)\n",
      "z est reparti :  tensor([0.4615, 0.6165, 0.7363, 0.7553, 0.7701, 0.7819, 0.7922, 0.8025, 0.8120,\n",
      "        0.8203], dtype=torch.float64) 392151151872 641958210048\n",
      "tensor([ 290,  481,  393,  616,  318,  674, 3503,  460,  356,  262])\n",
      "==========================================\n",
      "==========================================\n",
      "1  husband, wife, our daughter (you won't like her,) will\n",
      "z est reparti :  tensor([0.2196, 0.3062, 0.3478, 0.3865, 0.4176, 0.4459, 0.4711, 0.4954, 0.5171,\n",
      "        0.5379], dtype=torch.float64) 422470438624 1042106063456\n",
      "tensor([ 307, 1282,  407,  423,  766,  651, 1011, 1064,  467, 3187])\n",
      "==========================================\n",
      "==========================================\n",
      "3  husband, wife, our daughter (you won't like her,) will have\n",
      "z est reparti :  tensor([0.3905, 0.5024, 0.5836, 0.6376, 0.6589, 0.6743, 0.6886, 0.7016, 0.7144,\n",
      "        0.7270], dtype=torch.float64) 312196982608 695481299424\n",
      "tensor([ 257,  617,  262,  281, 1223,  517, 1576,  640, 1194,  284])\n",
      "==========================================\n",
      "==========================================\n",
      "815  husband, wife, our daughter (you won't like her,) will have grandparents\n",
      "z est reparti :  tensor([0.1551, 0.2464, 0.3358, 0.3884, 0.4401, 0.4898, 0.5348, 0.5676, 0.6001,\n",
      "        0.6304], dtype=torch.float64) 516810219520 591241854976\n",
      "tensor([287, 290, 508,  13, 612,  11, 393, 351, 284, 994])\n",
      "==========================================\n",
      "==========================================\n",
      "59  husband, wife, our daughter (you won't like her,) will have grandparents'\n",
      "z est reparti :  tensor([0.0659, 0.1143, 0.1576, 0.1961, 0.2268, 0.2528, 0.2749, 0.2964, 0.3120,\n",
      "        0.3275], dtype=torch.float64) 548983039088 550440036544\n",
      "tensor([ 1751, 28986,   290, 11864,   393,  5986,  1104,  3988,  1037,  3891])\n",
      "==========================================\n",
      "==========================================\n",
      "44  husband, wife, our daughter (you won't like her,) will have grandparents' information\n",
      "z est reparti :  tensor([0.1572, 0.2669, 0.3347, 0.3897, 0.4430, 0.4880, 0.5307, 0.5709, 0.6043,\n",
      "        0.6353], dtype=torch.float64) 549751462509 549755815926\n",
      "tensor([546,  13, 319, 290,  11, 329, 355, 287, 326, 284])\n",
      "==========================================\n",
      "==========================================\n",
      "5464  husband, wife, our daughter (you won't like her,) will have grandparents' information ware\n",
      "z est reparti :  tensor([0.1539, 0.2276, 0.2911, 0.3493, 0.3877, 0.4140, 0.4402, 0.4634, 0.4843,\n",
      "        0.5041], dtype=torch.float64) 549755813887 549755813889\n",
      "tensor([ 13,  11, 329, 290, 287, 284, 351, 878, 526, 546])\n",
      "==========================================\n",
      "==========================================\n",
      "1  husband, wife, our daughter (you won't like her,) will have grandparents' information ware,\n",
      "msg str:  [5229, 11, 3656, 11, 674, 4957, 357, 5832, 1839, 470, 588, 607, 35751, 481, 423, 28571, 6, 1321, 16202, 11] [5229, 11, 3656, 11, 674, 4957, 357, 5832, 1839, 470, 588, 607, 35751, 481, 423, 28571, 6, 1321, 16202, 11]\n",
      "stegotexte:   husband, wife, our daughter (you won't like her,) will have grandparents' information ware,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(message_secret)\n",
    "stegotexte = bit2str(bit_message, tokenizer, model, context=amorce)\n",
    "print(\"stegotexte: \", stegotexte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   40,   716,  1016,   319,   257, 14600,   284,  8031,    13,   314,\n",
      "          716,  7725,   326,   616])\n",
      "9.094947017729282e-13\n",
      "[0, 0, 1, 0]\n",
      "tensor([5229])\n",
      "2.0218037663063934e-12\n",
      "[0, 1, 1, 0, 0]\n",
      "tensor([11])\n",
      "1.4407736024146074e-12\n",
      "[0, 1, 1, 0, 1, 1]\n",
      "tensor([3656])\n",
      "2.6915208802903317e-12\n",
      "[]\n",
      "tensor([11])\n",
      "5.7838429002560465e-12\n",
      "[1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1]\n",
      "tensor([674])\n",
      "2.207197579288877e-12\n",
      "[]\n",
      "tensor([4957])\n",
      "2.257344000861651e-11\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "tensor([357])\n",
      "3.1225617738683937e-12\n",
      "[1, 0, 0, 1, 1, 1, 1, 0, 1]\n",
      "tensor([5832])\n",
      "2.6590907208698316e-12\n",
      "[1, 0, 0]\n",
      "tensor([1839])\n",
      "6.468492242599357e-11\n",
      "[]\n",
      "tensor([470])\n",
      "6.494004150972388e-11\n",
      "[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "tensor([588])\n",
      "2.4171514166244248e-12\n",
      "[0]\n",
      "tensor([607])\n",
      "6.4577819278929175e-12\n",
      "[1, 0, 0, 0, 0, 0, 0, 0]\n",
      "tensor([35751])\n",
      "4.003089453523191e-12\n",
      "[0, 1, 1, 1]\n",
      "tensor([481])\n",
      "1.61385168948465e-12\n",
      "[1, 0, 0, 1]\n",
      "tensor([423])\n",
      "2.609029266595485e-12\n",
      "[1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1]\n",
      "tensor([28571])\n",
      "1.3435147486328531e-11\n",
      "[1, 0, 0, 0]\n",
      "tensor([6])\n",
      "6.863429966071266e-10\n",
      "[]\n",
      "tensor([1321])\n",
      "2.2970462053141246e-07\n",
      "[]\n",
      "tensor([16202])\n",
      "0.5\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "tensor([11])\n",
      "1.8189894035458565e-12\n",
      "[0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0]\n",
      "tensor([27])\n",
      "2.5907952339337573e-12\n",
      "[1, 0, 0, 1, 1, 0, 1, 1, 1]\n",
      "tensor([68])\n",
      "2.512011572797606e-12\n",
      "[1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]\n",
      "tensor([418])\n",
      "1.7375842613046741e-12\n",
      "[0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "bit message:  bitarray('001001100011011100101110101011111111010011110110010000001000001000000001111001100111110001110001000000000000000000000000000000000000000111110111111100101001101111010000010001010001100111001110011100111000000000000000')\n"
     ]
    }
   ],
   "source": [
    "bit_message_new = str2bit(stegotexte, tokenizer, model, context=amorce)\n",
    "print(\"bit message: \", bit_message_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am hidden\n",
      "z est reparti :  tensor([0.0625, 0.1003, 0.1245, 0.1439, 0.1623, 0.1739, 0.1852, 0.1944, 0.2031,\n",
      "        0.2109], dtype=torch.float64) 0 1099511627776\n",
      "tensor([ 198,  464,    1,   32,   40,  818,   13, 1026,   50, 1212])\n",
      "==========================================\n",
      "==========================================\n",
      "4 I\n",
      "z est reparti :  tensor([0.0934, 0.1815, 0.2446, 0.2920, 0.3389, 0.3677, 0.3938, 0.4166, 0.4380,\n",
      "        0.4543], dtype=torch.float64) 332922693888 656147595392\n",
      "tensor([1101, 1053,  423,  716,  373,  836,  892,  550,  760,  460])\n",
      "==========================================\n",
      "==========================================\n",
      "3 I am\n",
      "z est reparti :  tensor([0.1394, 0.2198, 0.2520, 0.2804, 0.3054, 0.3279, 0.3497, 0.3712, 0.3910,\n",
      "        0.4082], dtype=torch.float64) 548452273732 609675909336\n",
      "tensor([ 257,  407, 1016,  281,  523, 3597,  262,  845, 3058, 1654])\n",
      "==========================================\n",
      "==========================================\n",
      "7330 I am hidden\n",
      "z est reparti :  tensor([0.2105, 0.3517, 0.4276, 0.4980, 0.5351, 0.5698, 0.6009, 0.6282, 0.6550,\n",
      "        0.6795], dtype=torch.float64) 279390978048 554197581824\n",
      "tensor([ 287,  422, 2157, 1497,  416,  739, 2641,  319,   11,   13])\n",
      "==========================================\n",
      "==========================================\n",
      "2706 I am hidden<\n",
      "z est reparti :  tensor([0.2927, 0.4142, 0.4616, 0.4878, 0.5112, 0.5293, 0.5439, 0.5578, 0.5697,\n",
      "        0.5809], dtype=torch.float64) 530612748288 913864392704\n",
      "tensor([   18,  1671,    29,    72,  3784,   198, 37330,   438,    64,   368])\n",
      "==========================================\n",
      "==========================================\n",
      "225 I am hidden<e\n",
      "z est reparti :  tensor([0.3167, 0.3583, 0.3840, 0.4081, 0.4299, 0.4501, 0.4645, 0.4775, 0.4903,\n",
      "        0.5018], dtype=torch.float64) 399087726592 597209028608\n",
      "tensor([   29,    13,  6927,    25,    12,   312, 28401,  1659, 10671,    11])\n",
      "==========================================\n",
      "==========================================\n",
      "159 I am hidden<eos\n",
      "z est reparti :  tensor([0.3928, 0.4442, 0.4793, 0.5142, 0.5474, 0.5782, 0.6026, 0.6238, 0.6347,\n",
      "        0.6435], dtype=torch.float64) 261180235776 1052628369408\n",
      "tensor([   29,    13,    12, 28401, 22330,    62,    25,    11, 31175,  6927])\n",
      "==========================================\n",
      "==========================================\n",
      "msg str:  [40, 716, 7104, 27, 68, 418, 29] [40, 716, 7104, 27, 68, 418, 29]\n",
      "decoded message:  I am hidden<eos>\n"
     ]
    }
   ],
   "source": [
    "print(message_secret)\n",
    "decoded_message = bit2str(bit_message, tokenizer, model)\n",
    "print(\"decoded message: \", decoded_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ArithmeticDecoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a3f5e15804b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArithmeticDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdecoded_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoded_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decoded message: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoded_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ArithmeticDecoder' is not defined"
     ]
    }
   ],
   "source": [
    "decoder = ArithmeticDecoder(model=model, tokenizer=tokenizer, topk=60000)\n",
    "decoded_message = decoder.decode(encoded_message, context=encoded_context)\n",
    "print(\"decoded message: \",decoded_message)\n",
    "print(tokenizer.decode(decoded_message))\n",
    "\n",
    "encoder = ArithmeticEncoder(model=model, tokenizer=tokenizer, topk=60000)\n",
    "encoded_message = encoder.encode(message_secret, contexte=encoded_context)\n",
    "print(\"encoded message: \",encoded_message)\n",
    "\n",
    "decoder = ArithmeticDecoder(model=model, tokenizer=tokenizer, topk=60000)\n",
    "decoded_message = decoder.decode(encoded_message)\n",
    "print(\"decoded message: \",decoded_message)\n",
    "print(tokenizer.decode(decoded_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(amorce, message_secret)\n",
    "#bit2str(encoded_message, model, tokenizer)[:-5]\n",
    "#str2bits(model=model, enc=tokenizer, message=message_secret)\n",
    "#decoded_message = bit2str(encoded_message, model, tokenizer)[:-5]\n",
    "encoded_message = str2bits(model=model, enc=tokenizer, message=message_secret)\n",
    "print(encoded_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sent_finish(token_idx, enc):\n",
    "    token = enc.decode(token_idx)\n",
    "    print(token)\n",
    "    return '.' in token or '!' in token or '?' in token\n",
    "\n",
    "def StegaGen(method, tokenizer, model, proc_seed): \n",
    "    \n",
    "    if method == 'arithmetic':\n",
    "        encoder_decoder = ArithmeticStega(tokenizer=tokenizer, model=model, seed=proc_seed) \n",
    "        \n",
    "    elif method == 'adg':\n",
    "        encoder_decoder = ArithmeticStega(tokenizer=tokenizer,model=model, seed=proc_seed)   \n",
    "    else:\n",
    "        print('Unkown method: ', method)\n",
    "        \n",
    "    return(encoder_decoder)\n",
    "\n",
    "\n",
    "class ArithmetiEncoder:\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.precision = kwargs.get(\"precision\", 32)\n",
    "        self.seed      = kwargs.get(\"seed\", None)\n",
    "        self.topk      = kwargs.get(\"topk\", 10)\n",
    "        self.topp      = kwargs.get(\"topp\", None)\n",
    "        self.model     = kwargs.get(\"model\")\n",
    "        \n",
    "        self.g         = torch.Generator(device=\"cpu\")\n",
    "        if self.seed is None:\n",
    "            self.g.seed()\n",
    "        else:\n",
    "            self.g.manual_seed(self.seed)\n",
    "        pass\n",
    "    \n",
    "    def reset(self, context):\n",
    "        return self.step(context, None)\n",
    "    \n",
    "    def reajust_probs(self, logits, indices):\n",
    "        probs = F.softmax(logits , dim=0)\n",
    "        if self.topp is not None:\n",
    "            overfill_index = ((probs.cumsum(0) > self.topp).nonzero())[0]\n",
    "            reajusted_probs = probs[:overfill_index]\n",
    "            reajusted_indices = indices[:overfill_index]\n",
    "        elif self.topk is not None:\n",
    "            reajusted_probs = probs[:self.topk]\n",
    "            reajusted_indices = indices[:self.topk]\n",
    "        return(F.softmax(reajusted_probs , dim=0), reajusted_indices)\n",
    "    \n",
    "    def step(self, current, past):\n",
    "        outputs = self.model(current.unsqueeze(0), past_key_values=past)\n",
    "        logits, past_keys = outputs.logits, outputs.past_key_values\n",
    "        \n",
    "        logits[0, -1, -1] = -1e20  # endoftext token can't happen\n",
    "        logits[0, -1, 628] = -1e20  # 2 newlines token can't happen\n",
    "        logits, indices = logits[0, -1, :].sort(descending=True)\n",
    "        \n",
    "        probs, indices = self.reajust_probs(logits, indices)\n",
    "        \n",
    "        return(probs, past_keys, indices)\n",
    "        \n",
    "    def encode(self, private_message_bit: bitarray.bitarray, context: str = None):\n",
    "        \"\"\"\n",
    "        :param msg: np array of 0s and 1s constituting a message bitstring\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        message = private_message_bit\n",
    "        tokenizer = self.tokenizer\n",
    "        precision = self.precision\n",
    "\n",
    "        max_val = 2 ** precision\n",
    "        cur_interval = [0, max_val]  # bottom inclusive, top exclusive\n",
    "\n",
    "        prev = context\n",
    "        print(\"context is \", context)\n",
    "        enc_prev = torch.LongTensor(tokenizer.encode(context))\n",
    "        \n",
    "        _, _, _ = self.reset(enc_prev)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            i = 0\n",
    "            j = 0\n",
    "            past=None\n",
    "            sent_finish = False\n",
    "            while i < len(message) or (not sent_finish):\n",
    "\n",
    "                probs, past, indices = self.step(enc_prev, past)\n",
    "                          \n",
    "                j += 1\n",
    "                probs_temp = probs\n",
    "\n",
    "                # conditions for having reached the end of the message\n",
    "                if i >= len(message):\n",
    "                    token = tokenizer.decode(token_idx)\n",
    "                    sent_finish = '.' in token or '!' in token or '?' in token\n",
    "                    print(\"token: \", token,\"sent_finish: \",  sent_finish)\n",
    "                else:\n",
    "                    # Cutoff low probabilities that would be rounded to 0\n",
    "                    cur_int_range = cur_interval[1] - cur_interval[0]\n",
    "\n",
    "                    # Rescale to correct range\n",
    "                    probs_temp_int = probs_temp * cur_int_range\n",
    "\n",
    "                    # Round probabilities to integers given precision\n",
    "                    probs_temp_int = probs_temp_int.round().long()\n",
    "                    cum_probs = probs_temp_int.cumsum(0)\n",
    "                    \n",
    "                    # Remove any elements from the bottom if rounding caused the total prob to be too large\n",
    "                    overfill_index = (cum_probs > cur_int_range).nonzero()\n",
    "                    print(overfill_index, \"uh\")\n",
    "                    if len(overfill_index) > 0:\n",
    "                        cum_probs = cum_probs[:overfill_index[0]]\n",
    "\n",
    "                    print(cum_probs[-1], \"ah\")\n",
    "                    # Add any mass to the top if removing/rounding causes the total prob to be too small\n",
    "                    cum_probs += cur_int_range - cum_probs[-1]  # add\n",
    "\n",
    "                    # Get out resulting probabilities\n",
    "                    probs_final = cum_probs.clone()\n",
    "                    probs_final[1:] = cum_probs[1:] - cum_probs[:-1]\n",
    "\n",
    "                    # Convert to position in range\n",
    "                    cum_probs += cur_interval[0]\n",
    "\n",
    "                    # Get selected index based on binary fraction from message bits\n",
    "                    message_bits = message[i:i + precision]\n",
    "                    if i + precision > len(message):\n",
    "                        message_bits = message_bits + [0] * (i + precision - len(message))\n",
    "                    message_idx = bits2int(reversed(message_bits))\n",
    "                    selection = (cum_probs > message_idx).nonzero()[0].item()\n",
    "                    \n",
    "                    # Calculate new range as ints\n",
    "                    new_int_bottom = cum_probs[selection - 1] if selection > 0 else cur_interval[0]\n",
    "                    new_int_top = cum_probs[selection]\n",
    "\n",
    "                    # Convert range to bits\n",
    "                    new_int_bottom_bits_inc = list(reversed(int2bits(new_int_bottom, precision)))\n",
    "                    new_int_top_bits_inc = list(reversed(int2bits(new_int_top - 1, precision)))  \n",
    "                    # -1 here because upper bound is exclusive\n",
    "\n",
    "                    # Consume most significant bits which are now fixed and update interval\n",
    "                    num_bits_encoded = num_same_from_beg(new_int_bottom_bits_inc, new_int_top_bits_inc)\n",
    "                    i += num_bits_encoded\n",
    "\n",
    "                    new_int_bottom_bits = new_int_bottom_bits_inc[num_bits_encoded:] + [0] * num_bits_encoded\n",
    "                    new_int_top_bits = new_int_top_bits_inc[num_bits_encoded:] + [1] * num_bits_encoded\n",
    "\n",
    "                    cur_interval[0] = bits2int(reversed(new_int_bottom_bits))\n",
    "                    cur_interval[1] = bits2int(reversed(new_int_top_bits)) + 1  \n",
    "                    # +1 here because upper bound is exclusive\n",
    "\n",
    "                    q = probs_final.double() / probs_final.sum()\n",
    "\n",
    "                # Update history with new token\n",
    "                prev = indices[selection].view(1)\n",
    "                \n",
    "                enc_prev = torch.cat((enc_prev, prev))\n",
    "                print(\"current message: \", tokenizer.decode(enc_prev), prev, selection)\n",
    "\n",
    "                # For text->bits->text\n",
    "                partial = self.tokenizer.decode(enc_prev[len(context):].tolist())\n",
    "                if '<eos>' in partial:\n",
    "                    break\n",
    "\n",
    "\n",
    "        return self.medium.enc.decode(enc_prev[len(context):].tolist()), enc_prev[len(context):].tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = -1\n",
    "proc_seed = (seed * 55555) % 77777\n",
    "proc_rng = np.random.default_rng(proc_seed)\n",
    "\n",
    "# Now branch into different methods\n",
    "encoder_decoder = StegaGen('arithmetic', tokenizer, model, proc_seed)\n",
    "\n",
    "print(encoded_message, amorce)\n",
    "ah = model( torch.LongTensor(tokenizer.encode(amorce)).unsqueeze(0)  )\n",
    "logits=  ah.logits\n",
    "print(logits.argmax())\n",
    "\n",
    "# Proceed with communication\n",
    "public_message_str, public_message_token = encoder_decoder.encode(private_message_bit=encoded_message,context=amorce)\n",
    "print(public_message_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def decode(self, public_message_token, text=None, context: str = None, private_message_bitlen=None, **kwargs):\n",
    "\n",
    "        inp = public_message_token\n",
    "        enc = self.medium.enc\n",
    "        device = self.medium.device  # may want to restrict to CPU!\n",
    "        precision = self.precision\n",
    "        max_val = 2 ** precision\n",
    "\n",
    "        max_val = 2 ** precision\n",
    "        cur_interval = [0, max_val]  # bottom inclusive, top exclusive\n",
    "        \n",
    "        t_iter_1 = None\n",
    "\n",
    "        enc_context = self.medium.encode_context(context)\n",
    "        enc_context = th.tensor(enc_context[-1022:], device=device, dtype=th.long)\n",
    "        prev = enc_context\n",
    "        message = []\n",
    "        with th.no_grad():\n",
    "            i = 0\n",
    "            j = 0\n",
    "            while i < len(inp):\n",
    "\n",
    "                if t_iter_1 is not None:\n",
    "                    delta_t_step_no_medium = time.time() - t_iter_1\n",
    "                t_medium_1 = time.time()\n",
    "\n",
    "                if j == 0:\n",
    "                    probs, info = self.medium.reset(context=context)\n",
    "                else:\n",
    "                    probs, info = self.medium.step(prev)\n",
    "                j += 1\n",
    "                probs = th.from_numpy(probs.astype(np.float64))\n",
    "\n",
    "                delta_t_medium = time.time() - t_medium_1\n",
    "                t_iter_1 = time.time()\n",
    "\n",
    "                probs_temp = probs\n",
    "                indices = th.from_numpy(info[\"indices\"])\n",
    "                probs_temp_int = probs_temp\n",
    "\n",
    "                # Cutoff low probabilities that would be rounded to 0\n",
    "                cur_int_range = cur_interval[1] - cur_interval[0]\n",
    "\n",
    "                # Rescale to correct range\n",
    "                probs_temp_int = probs_temp_int / probs_temp_int.sum() * cur_int_range\n",
    "\n",
    "                # Round probabilities to integers given precision\n",
    "                probs_temp_int = probs_temp_int.round().long()\n",
    "                cum_probs = probs_temp_int.cumsum(0)\n",
    "\n",
    "                # Remove any elements from the bottom if rounding caused the total prob to be too large\n",
    "                overfill_index = (cum_probs > cur_int_range).nonzero()\n",
    "                if len(overfill_index) > 0:\n",
    "                    cum_probs = cum_probs[:overfill_index[0]]\n",
    "                    k = overfill_index[0].item()\n",
    "\n",
    "                # Add any mass to the top if removing/rounding causes the total prob to be too small\n",
    "                cum_probs += cur_int_range - cum_probs[-1]  # add\n",
    "\n",
    "                # Covnert to position in range\n",
    "                cum_probs += cur_interval[0]\n",
    "\n",
    "                rank = (indices == inp[i]).nonzero().item()\n",
    "\n",
    "                k = len(probs)\n",
    "                # Handle most errors that could happen because of BPE with heuristic\n",
    "                if rank >= k:\n",
    "                    true_token_text = enc.decoder[inp[i]]\n",
    "                    for rank_idx in range(k):\n",
    "                        prop_token_text = enc.decoder[indices[rank_idx].item()]\n",
    "                        # common case that is not caught\n",
    "                        if inp[i] == 128 and indices[rank_idx] == 198:\n",
    "                            rank = rank_idx\n",
    "                            inp[i] = indices[rank_idx].item()\n",
    "                            break\n",
    "\n",
    "                        # Is there a more likely prefix token that could be the actual token generated?\n",
    "                        if len(prop_token_text) <= len(true_token_text) and \\\n",
    "                                prop_token_text == true_token_text[:len(prop_token_text)]:\n",
    "                            rank = rank_idx\n",
    "                            suffix = true_token_text[len(prop_token_text):]\n",
    "                            suffix_tokens = enc.encode(suffix)  # a list\n",
    "                            inp[i] = indices[rank_idx].item()\n",
    "                            inp[i + 1:i + 1] = suffix_tokens  # insert suffix tokens into list\n",
    "                            break\n",
    "\n",
    "                        # Is there a more likely longer token that could be the actual token generated?\n",
    "                        elif len(prop_token_text) > len(true_token_text) and \\\n",
    "                                true_token_text == prop_token_text[:len(true_token_text)]:\n",
    "                            whole_text = true_token_text\n",
    "                            num_extra = 1\n",
    "                            while len(whole_text) < len(prop_token_text):\n",
    "                                whole_text += enc.decoder[inp[i + num_extra]]\n",
    "                                num_extra += 1\n",
    "                            if prop_token_text == whole_text[:len(prop_token_text)]:\n",
    "                                rank = rank_idx\n",
    "                                inp[i] = indices[rank_idx].item()\n",
    "                                for j in range(1, num_extra):\n",
    "                                    del inp[i + j]\n",
    "\n",
    "                                if len(whole_text) > len(prop_token_text):\n",
    "                                    suffix = whole_text[len(prop_token_text):]\n",
    "                                    suffix_tokens = enc.encode(suffix)  # a list\n",
    "                                    inp[i + 1:i + 1] = suffix_tokens  # insert suffix tokens into list\n",
    "                                break\n",
    "                    else:\n",
    "                        print('Unable to fix BPE error: token received: %s=%d, text: %s' % (\n",
    "                            true_token_text, inp[i], text))\n",
    "                        rank = 0\n",
    "\n",
    "                selection = rank\n",
    "\n",
    "                # Calculate new range as ints\n",
    "                new_int_bottom = cum_probs[selection - 1] if selection > 0 else cur_interval[0]\n",
    "                new_int_top = cum_probs[selection]\n",
    "\n",
    "                # Convert range to bits\n",
    "                new_int_bottom_bits_inc = list(reversed(int2bits(new_int_bottom, precision)))\n",
    "                new_int_top_bits_inc = list(reversed(int2bits(new_int_top - 1, precision)))  \n",
    "                # -1 because upper bound is exclusive\n",
    "\n",
    "                # Emit most significant bits which are now fixed and update interval\n",
    "                num_bits_encoded = num_same_from_beg(new_int_bottom_bits_inc, new_int_top_bits_inc)\n",
    "                if i == len(inp) - 1:\n",
    "                    new_bits = new_int_bottom_bits_inc\n",
    "                else:\n",
    "                    new_bits = new_int_top_bits_inc[:num_bits_encoded]\n",
    "                message += new_bits\n",
    "\n",
    "                new_int_bottom_bits = new_int_bottom_bits_inc[num_bits_encoded:] + [0] * num_bits_encoded\n",
    "                new_int_top_bits = new_int_top_bits_inc[num_bits_encoded:] + [1] * num_bits_encoded\n",
    "\n",
    "                cur_interval[0] = bits2int(reversed(new_int_bottom_bits))\n",
    "                cur_interval[1] = bits2int(reversed(new_int_top_bits)) + 1  \n",
    "                # +1 because upper bound is exclusive\n",
    "\n",
    "                # Update history with new token\n",
    "                prev = th.tensor([inp[i]], device=device, dtype=th.long)\n",
    "                i += 1\n",
    "\n",
    "        output = bitarray.bitarray(message)\n",
    "        if private_message_bitlen is not None:\n",
    "            output = output[:private_message_bitlen]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
