{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Génération de Texte sous contrainte stéganographiqe</h1>\n",
    "\n",
    "Dans ce notebook, nous étudions le codage arithmétique comme méthode d'insertion stéganographique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import bitarray\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation des différentes variables et nom du modèle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_secret = \"I am hidden\"\n",
    "message_secret = message_secret + '<eos>'\n",
    "\n",
    "amorce = \"I am giving a lecture to my wonderful\"\n",
    "\n",
    "model_name = \"gpt2-large\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger modèle et tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodage du message et de l'amorce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amorce: I am giving a lecture to my wonderful \n",
      "encodée en: [40, 716, 3501, 257, 19143, 284, 616, 7932]\n",
      "\n",
      "message_secret: I am hidden<eos> \n",
      "encodée en: [40, 716, 7104, 27, 68, 418, 29]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_message = tokenizer.encode(message_secret)\n",
    "encoded_context = tokenizer.encode(amorce)\n",
    "\n",
    "print(\"amorce: {} \\nencodée en: {}\\n\".format(amorce, encoded_context))\n",
    "print(\"message_secret: {} \\nencodée en: {}\\n\".format(message_secret, encoded_message))\n",
    "\n",
    "tensor_amorce = torch.LongTensor(encoded_context).view(1,-1)\n",
    "tensor_message = torch.LongTensor(encoded_message).view(1,-1)\n",
    "\n",
    "#liste des tokens visibles sur:\n",
    "#https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple de séquences générées sans contrainte stéga:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exemple 0: I am giving a lecture to my wonderful students... about the value of hard work...\"\n",
      "\n",
      "On the other side of the debate,\n",
      "\n",
      "...there's a group of guys that are pretty,\n",
      "\n",
      "exemple 1: I am giving a lecture to my wonderful students at the same time. I cannot speak very fast and at the same time I cannot move my body.\"\n",
      "\n",
      "He said, \"I am very weak\n",
      "\n",
      "exemple 2: I am giving a lecture to my wonderful class on the history of how women used to be treated in America, and I am going to say the words: 'It's a feminist who'll kick your\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_output = model.generate(tensor_amorce, do_sample=True, max_length=40,\\\n",
    "                                 top_k=50, top_p=1, temperature=1, num_return_sequences=3)\n",
    "\n",
    "for i in range(sampling_output.shape[0]):\n",
    "        print(\"exemple {}: {}\\n\".format(i,tokenizer.decode(sampling_output[i].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code pour l'encodage arithmétique (de texte à bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_arithmetic(model, enc, text, context, device='cuda', temp=1.0, precision=16, topk=50000):\n",
    "    # inp is a list of token indices\n",
    "    # context is a list of token indices\n",
    "    inp = enc.encode(text)\n",
    "    # common BPE error case: 128, 128 (2 newlines) is interpretted as 628 (2 newlines)\n",
    "    i = 0\n",
    "    while i < len(inp):\n",
    "        if inp[i] == 628:\n",
    "            inp[i] = 198\n",
    "            inp[i + 1:i + 1] = [198]\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    context = torch.tensor(context[-1022:], device=device, dtype=torch.long)\n",
    "\n",
    "    max_val = 2 ** precision\n",
    "    threshold = 2 ** (-precision)\n",
    "    cur_interval = [0, max_val]  # bottom inclusive, top exclusive\n",
    "\n",
    "    prev = context\n",
    "    past = None\n",
    "    message = []\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        while i < len(inp):\n",
    "            outputs = model(prev.unsqueeze(0), past_key_values=past)\n",
    "            past, logits = outputs.past_key_values, outputs.logits\n",
    "            \n",
    "            logits[0, -1, -1] = -1e10  # endoftext can't happen\n",
    "            logits[0, -1, 628] = -1e10  # 2 newlines can't happen\n",
    "            logits, indices = logits[0, -1, :].sort(descending=True)\n",
    "            logits = logits.double()\n",
    "            logits_temp = logits / temp\n",
    "            probs_temp = F.softmax(logits_temp, dim=0)\n",
    "            cum_probs = probs_temp.cumsum(0)\n",
    "\n",
    "            # Cutoff low probabilities that would be rounded to 0\n",
    "            cur_int_range = cur_interval[1] - cur_interval[0]\n",
    "            cur_threshold = 1 / cur_int_range\n",
    "            \n",
    "            k = min(max(2, (probs_temp < cur_threshold).nonzero()[0].item()), topk)\n",
    "            probs_temp_int = probs_temp[:k]  # Cutoff all but top k\n",
    "\n",
    "            # Rescale to correct range\n",
    "            probs_temp_int = probs_temp_int / probs_temp_int.sum() * cur_int_range\n",
    "\n",
    "            # Round probabilities to integers given precision\n",
    "            probs_temp_int = probs_temp_int.round().long()\n",
    "            cum_probs = probs_temp_int.cumsum(0)\n",
    "\n",
    "            # Remove any elements from the bottom if rounding caused the total prob to be too large\n",
    "            overfill_index = (cum_probs > cur_int_range).nonzero()\n",
    "            if len(overfill_index) > 0:\n",
    "                cum_probs = cum_probs[:overfill_index[0]]\n",
    "                k = overfill_index[0].item()\n",
    "\n",
    "            # Add any mass to the top if removing/rounding causes the total prob to be too small\n",
    "            cum_probs += cur_int_range - cum_probs[-1]  # add\n",
    "\n",
    "            # Covnert to position in range\n",
    "            cum_probs += cur_interval[0]\n",
    "\n",
    "            rank = (indices == inp[i]).nonzero().item()\n",
    "\n",
    "            # Handle most errors that could happen because of BPE with heuristic\n",
    "            if rank >= k:\n",
    "                true_token_text = enc.decode(inp[i])\n",
    "                for rank_idx in range(k):\n",
    "                    prop_token_text = enc.decode(indices[rank_idx].item())\n",
    "                    # common case that is not caught\n",
    "                    if inp[i] == 128 and indices[rank_idx] == 198:\n",
    "                        rank = rank_idx\n",
    "                        inp[i] = indices[rank_idx].item()\n",
    "                        break\n",
    "\n",
    "                    # Is there a more likely prefix token that could be the actual token generated?\n",
    "                    if len(prop_token_text) <= len(true_token_text) and \\\n",
    "                            prop_token_text == true_token_text[:len(prop_token_text)]:\n",
    "                        rank = rank_idx\n",
    "                        suffix = true_token_text[len(prop_token_text):]\n",
    "                        suffix_tokens = enc.encode(suffix)  # a list\n",
    "                        inp[i] = indices[rank_idx].item()\n",
    "                        inp[i + 1:i + 1] = suffix_tokens  # insert suffix tokens into list\n",
    "                        break\n",
    "\n",
    "                    # Is there a more likely longer token that could be the actual token generated?\n",
    "                    elif len(prop_token_text) > len(true_token_text) and \\\n",
    "                            true_token_text == prop_token_text[:len(true_token_text)]:\n",
    "                        whole_text = true_token_text\n",
    "                        num_extra = 1\n",
    "                        while len(whole_text) < len(prop_token_text):\n",
    "                            whole_text += enc.decode(inp[i + num_extra])\n",
    "                            num_extra += 1\n",
    "                        if prop_token_text == whole_text[:len(prop_token_text)]:\n",
    "                            rank = rank_idx\n",
    "                            inp[i] = indices[rank_idx].item()\n",
    "                            for j in range(1, num_extra):\n",
    "                                del inp[i + j]\n",
    "\n",
    "                            if len(whole_text) > len(prop_token_text):\n",
    "                                suffix = whole_text[len(prop_token_text):]\n",
    "                                suffix_tokens = enc.encode(suffix)  # a list\n",
    "                                inp[i + 1:i + 1] = suffix_tokens  # insert suffix tokens into list\n",
    "                            break\n",
    "                else:\n",
    "                    #print('Unable to fix BPE error: token received: %s=%d, text: %s' % (true_token_text, inp[i], text))\n",
    "                    rank = 0\n",
    "\n",
    "            selection = rank\n",
    "\n",
    "            # Calculate new range as ints\n",
    "            new_int_bottom = cum_probs[selection - 1] if selection > 0 else cur_interval[0]\n",
    "            new_int_top = cum_probs[selection]\n",
    "\n",
    "            # Convert range to bits\n",
    "            new_int_bottom_bits_inc = list(reversed(int2bits(new_int_bottom, precision)))\n",
    "            new_int_top_bits_inc = list(\n",
    "                reversed(int2bits(new_int_top - 1, precision)))  # -1 here because upper bound is exclusive\n",
    "\n",
    "            # Emit most significant bits which are now fixed and update interval\n",
    "            num_bits_encoded = num_same_from_beg(new_int_bottom_bits_inc, new_int_top_bits_inc)\n",
    "            if i == len(inp) - 1:\n",
    "                new_bits = new_int_bottom_bits_inc\n",
    "            else:\n",
    "                new_bits = new_int_top_bits_inc[:num_bits_encoded]\n",
    "            message += new_bits\n",
    "\n",
    "            print(\"new bits: \", new_bits)\n",
    "            new_int_bottom_bits = new_int_bottom_bits_inc[num_bits_encoded:] + [0] * num_bits_encoded\n",
    "            new_int_top_bits = new_int_top_bits_inc[num_bits_encoded:] + [1] * num_bits_encoded\n",
    "            cur_interval[0] = bits2int(reversed(new_int_bottom_bits))\n",
    "            cur_interval[1] = bits2int(reversed(new_int_top_bits)) + 1  # +1 here because upper bound is exclusive\n",
    "\n",
    "            # Update history with new token\n",
    "            prev = torch.tensor([inp[i]], device=device, dtype=torch.long)\n",
    "            i += 1\n",
    "            \n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code pour le décodage arithmétique (de bits à du texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_arithmetic(model, enc, message, context, finish_sent=False, model_device=\"cuda\", device='cpu', temp=1.0, precision=16,\n",
    "                      topk=50000):\n",
    "    context = torch.tensor(context[-1022:], device=device, dtype=torch.long)\n",
    "\n",
    "    max_val = 2 ** precision\n",
    "    threshold = 2 ** (-precision)\n",
    "    cur_interval = [0, max_val]  # bottom inclusive, top exclusive\n",
    "\n",
    "    prev = context\n",
    "    output = context\n",
    "    past = None\n",
    "\n",
    "    total_num = 0\n",
    "    total_num_for_stats = 0\n",
    "    total_log_probs = 0\n",
    "    total_kl = 0  # in bits\n",
    "    total_entropy_ptau = 0\n",
    "    total_num_sents = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        sent_finish = False\n",
    "        while i < len(message) or (finish_sent and not sent_finish):\n",
    "            outputs = model(prev.unsqueeze(0).to(model_device), past_key_values=past)\n",
    "            logits, past = outputs.logits, outputs.past_key_values\n",
    "            logits = logits.to(device)\n",
    "            \n",
    "            logits[0, -1, -1] = -1e20  # endoftext token can't happen\n",
    "            logits[0, -1, 628] = -1e20  # 2 newlines token can't happen\n",
    "            logits, indices = logits[0, -1, :].sort(descending=True)\n",
    "            logits = logits.double()\n",
    "            logits_temp = logits / temp\n",
    "            probs_temp = F.softmax(logits_temp, dim=0)\n",
    "            log_probs_temp = F.log_softmax(logits_temp, dim=0)\n",
    "            log_probs = F.log_softmax(logits, dim=0)\n",
    "\n",
    "            # conditions for having reached the end of the message\n",
    "            if i >= len(message):\n",
    "                selection = 0\n",
    "                sent_finish = is_sent_finish(indices[selection].item(), enc)\n",
    "            else:\n",
    "                # Cutoff low probabilities that would be rounded to 0\n",
    "                cur_int_range = cur_interval[1] - cur_interval[0]\n",
    "                cur_threshold = 1 / cur_int_range\n",
    "                k = min(max(2, (probs_temp < cur_threshold).nonzero()[0].item()), topk)\n",
    "                probs_temp_int = probs_temp[:k]  # Cutoff all but top k\n",
    "\n",
    "                # Rescale to correct range\n",
    "                probs_temp_int = probs_temp_int / probs_temp_int.sum() * cur_int_range\n",
    "\n",
    "                # Round probabilities to integers given precision\n",
    "                probs_temp_int = probs_temp_int.round().long()\n",
    "                cum_probs = probs_temp_int.cumsum(0)\n",
    "\n",
    "                # Remove any elements from the bottom if rounding caused the total prob to be too large\n",
    "                overfill_index = (cum_probs > cur_int_range).nonzero()\n",
    "                if len(overfill_index) > 0:\n",
    "                    cum_probs = cum_probs[:overfill_index[0]]\n",
    "\n",
    "                # Add any mass to the top if removing/rounding causes the total prob to be too small\n",
    "                cum_probs += cur_int_range - cum_probs[-1]  # add\n",
    "\n",
    "                # Get out resulting probabilities\n",
    "                probs_final = cum_probs.clone()\n",
    "                probs_final[1:] = cum_probs[1:] - cum_probs[:-1]\n",
    "\n",
    "                # Convert to position in range\n",
    "                cum_probs += cur_interval[0]\n",
    "\n",
    "                # Get selected index based on binary fraction from message bits\n",
    "                message_bits = message[i:i + precision]\n",
    "                if i + precision > len(message):\n",
    "                    message_bits = message_bits + [0] * (i + precision - len(message))\n",
    "                message_idx = bits2int(reversed(message_bits))\n",
    "                selection = (cum_probs > message_idx).nonzero()[0].item()\n",
    "\n",
    "                # Calculate new range as ints\n",
    "                new_int_bottom = cum_probs[selection - 1] if selection > 0 else cur_interval[0]\n",
    "                new_int_top = cum_probs[selection]\n",
    "\n",
    "                # Convert range to bits\n",
    "                new_int_bottom_bits_inc = list(reversed(int2bits(new_int_bottom, precision)))\n",
    "                new_int_top_bits_inc = list(\n",
    "                    reversed(int2bits(new_int_top - 1, precision)))  # -1 here because upper bound is exclusive\n",
    "                # Consume most significant bits which are now fixed and update interval\n",
    "                num_bits_encoded = num_same_from_beg(new_int_bottom_bits_inc, new_int_top_bits_inc)\n",
    "                i += num_bits_encoded\n",
    "\n",
    "                new_int_bottom_bits = new_int_bottom_bits_inc[num_bits_encoded:] + [0] * num_bits_encoded\n",
    "                new_int_top_bits = new_int_top_bits_inc[num_bits_encoded:] + [1] * num_bits_encoded\n",
    "\n",
    "                cur_interval[0] = bits2int(reversed(new_int_bottom_bits))\n",
    "                cur_interval[1] = bits2int(reversed(new_int_top_bits)) + 1  # +1 here because upper bound is exclusive\n",
    "\n",
    "\n",
    "            # Update history with new token\n",
    "            prev = indices[selection].view(1)\n",
    "            output = torch.cat((output, prev))\n",
    "            total_num += 1\n",
    "            \n",
    "\n",
    "            # For text->bits->text\n",
    "            partial = enc.decode(output[len(context):].tolist())\n",
    "            \n",
    "            if '<eos>' in partial:\n",
    "                break\n",
    "\n",
    "\n",
    "    return output[len(context):].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Déclaration de fonctions utiles pour la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def int2bits(inp, num_bits):\n",
    "    if num_bits == 0:\n",
    "        return []\n",
    "    strlist = ('{0:0%db}' % num_bits).format(inp)\n",
    "    return [int(strval) for strval in reversed(strlist)]\n",
    "\n",
    "def num_same_from_beg(bits1, bits2):\n",
    "    assert len(bits1) == len(bits2)\n",
    "    for i in range(len(bits1)):\n",
    "        if bits1[i] != bits2[i]:\n",
    "            break\n",
    "\n",
    "    return i\n",
    "\n",
    "def bits2int(bits):\n",
    "    res = 0\n",
    "    for i, bit in enumerate(bits):\n",
    "        res += bit * (2 ** i)\n",
    "    return res\n",
    "\n",
    "\n",
    "def str2bit(msg_str, tokenizer, model, context=None, topk=60000):\n",
    "    if context is None:\n",
    "        message_ctx = tokenizer.encode('<|endoftext|>')\n",
    "    else: message_ctx = tokenizer.encode(context)\n",
    "    msg_str += '<eos>'\n",
    "    msg_bits = bitarray.bitarray()\n",
    "    msg_enc = encode_arithmetic(model, tokenizer, msg_str, message_ctx,\n",
    "                                    precision=40, topk=topk, device='cpu')\n",
    "    msg_bits = bitarray.bitarray(msg_enc)\n",
    "        \n",
    "    return msg_bits\n",
    "\n",
    "def bit2str(msg_bits, tokenizer, model, context=None, topk=60000, finish_sent=False):\n",
    "    \n",
    "    if context is None:\n",
    "        message_ctx = tokenizer.encode('<|endoftext|>')\n",
    "    else: \n",
    "        message_ctx = tokenizer.encode(context)\n",
    "        \n",
    "    msg_str = decode_arithmetic(model, tokenizer, msg_bits, message_ctx,\n",
    "        precision=40, topk=topk, device=\"cpu\", model_device='cpu', finish_sent=finish_sent)\n",
    "    msg_str = tokenizer.decode(msg_str)\n",
    "    return msg_str\n",
    "\n",
    "\n",
    "def is_sent_finish(token_idx, enc):\n",
    "    token = enc.decode(token_idx)\n",
    "    return '.' in token or '!' in token or '?' in token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder et décoder le message secret pour l'insertion stégo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message secret à encoder: I am hidden\n",
      "\n",
      "new bits:  [0, 0, 1, 0, 1, 1]\n",
      "new bits:  [0, 1, 0]\n",
      "new bits:  [1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "new bits:  [1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1]\n",
      "new bits:  [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
      "new bits:  [0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0]\n",
      "new bits:  [0, 1, 1, 1, 1]\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  [0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "message encodé: bitarray('0010110101101100010101000110001110101100111000100100001110111010011110111111010110000110001110000000111000000')\n",
      "\n",
      "message décodé:  I am hidden\n"
     ]
    }
   ],
   "source": [
    "print(\"message secret à encoder: {}\\n\".format(message_secret[:message_secret.index('<eos>')]))\n",
    "bit_message = str2bit(message_secret, tokenizer, model)\n",
    "print(\"message encodé: {}\\n\".format(bit_message))\n",
    "decoded_message = bit2str(bit_message, tokenizer, model)\n",
    "print(\"message décodé: \", decoded_message[:decoded_message.index('<eos>')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stegotexte:  I am giving a lecture to my wonderful students. And I am also going to say to them, you may want to get back to you and ask your boss if it is permissible to talk to your colleagues. It is not necessary to go through the whole thing. But if you do, you will find that it is not permissible to talk to your colleagues.\n"
     ]
    }
   ],
   "source": [
    "stegotexte = bit2str(bit_message, tokenizer, model, context=amorce, topk=30, finish_sent=True)\n",
    "print(\"stegotexte: \", amorce+stegotexte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new bits:  [0]\n",
      "new bits:  [0, 1, 0]\n",
      "new bits:  [1, 1, 0, 1]\n",
      "new bits:  [0, 1, 1]\n",
      "new bits:  [0, 1]\n",
      "new bits:  [1, 0, 0, 0]\n",
      "new bits:  [1, 0, 1, 0]\n",
      "new bits:  []\n",
      "new bits:  [1, 0, 0, 0, 1]\n",
      "new bits:  [1, 0, 0]\n",
      "new bits:  [0, 1, 1, 1]\n",
      "new bits:  [0]\n",
      "new bits:  [1, 0, 1]\n",
      "new bits:  [1, 0, 0, 1, 1]\n",
      "new bits:  [1, 0, 0, 0, 1, 0]\n",
      "new bits:  []\n",
      "new bits:  [0]\n",
      "new bits:  [1, 0, 0, 0, 0, 1, 1, 1]\n",
      "new bits:  [0, 1]\n",
      "new bits:  [1, 1, 0, 1, 0]\n",
      "new bits:  [0, 1, 1, 1, 1]\n",
      "new bits:  []\n",
      "new bits:  [0, 1, 1, 1, 1, 1]\n",
      "new bits:  [1, 0, 1, 0, 1]\n",
      "new bits:  [1, 0, 0, 0, 0]\n",
      "new bits:  [1, 1, 0, 0]\n",
      "new bits:  [0, 1]\n",
      "new bits:  [1, 1, 0, 0, 0, 0, 0, 0]\n",
      "new bits:  []\n",
      "new bits:  [0, 1, 1]\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "new bits:  [0, 0]\n",
      "new bits:  [0, 0, 0]\n",
      "new bits:  []\n",
      "new bits:  [0, 0]\n",
      "new bits:  [0]\n",
      "new bits:  [0, 0]\n",
      "new bits:  [0]\n",
      "new bits:  [0, 0]\n",
      "new bits:  [0]\n",
      "new bits:  [0, 0]\n",
      "new bits:  []\n",
      "new bits:  [0, 0]\n",
      "new bits:  [0]\n",
      "new bits:  [0, 0]\n",
      "new bits:  []\n",
      "new bits:  [0]\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  [0, 0]\n",
      "new bits:  [0, 0]\n",
      "new bits:  [0]\n",
      "new bits:  [1, 1, 0, 0, 1, 1, 1]\n",
      "new bits:  []\n",
      "new bits:  [0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Bits encodés par le texte stego: bitarray('00101101011011000101010001100011101011001110001001000011101110100111101111110101100001100011100000001110000000000000000000000000000000000000000000000000000000000000000011001110010111011000110100011001010110000000000') \n",
      "\n",
      "Message décodé:  I am hidden\n"
     ]
    }
   ],
   "source": [
    "bit_message_new = str2bit(stegotexte, tokenizer, model, context=amorce, topk=30)\n",
    "print(\"Bits encodés par le texte stego: {} \\n\".format(bit_message_new))\n",
    "decoded_message = bit2str(bit_message_new, tokenizer, model)\n",
    "print(\"Message décodé: \", decoded_message[:decoded_message.index('<eos>')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new bits:  [0]\n",
      "new bits:  []\n",
      "new bits:  [0]\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  [0]\n",
      "new bits:  [1, 0, 1, 0, 1, 0, 1]\n",
      "new bits:  []\n",
      "new bits:  [0, 1, 1]\n",
      "new bits:  [1, 0, 0, 1, 0]\n",
      "new bits:  []\n",
      "new bits:  [0]\n",
      "new bits:  []\n",
      "new bits:  [1, 0, 0, 0, 1]\n",
      "new bits:  [0]\n",
      "new bits:  [0, 1, 1, 0]\n",
      "new bits:  [0, 1]\n",
      "new bits:  []\n",
      "new bits:  [1, 1, 0, 0, 1, 0]\n",
      "new bits:  [0, 1]\n",
      "new bits:  [0]\n",
      "new bits:  [1]\n",
      "new bits:  []\n",
      "new bits:  []\n",
      "new bits:  [1, 0, 0, 0, 0, 0, 1]\n",
      "new bits:  [0, 1]\n",
      "new bits:  [1, 1, 1, 1, 1, 1]\n",
      "new bits:  []\n",
      "new bits:  [1, 0, 1, 1, 0, 1, 1]\n",
      "new bits:  [0, 1, 0, 0]\n",
      "new bits:  [1]\n",
      "new bits:  [0, 1, 1]\n",
      "new bits:  [0, 1]\n",
      "new bits:  []\n",
      "new bits:  [1, 0, 0, 0, 1, 1, 1]\n",
      "new bits:  [1, 0, 1]\n",
      "new bits:  []\n",
      "new bits:  [1, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "new bits:  [1, 1, 1, 0, 0, 0]\n",
      "new bits:  [0, 1]\n",
      "new bits:  [0]\n",
      "new bits:  [1, 1, 0, 0, 0, 1, 1]\n",
      "new bits:  [0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Bits encodés par le texte stego: bitarray('0001010101011100100100010011001110010010110000010111111110110110100101101100011110110010000111100001011000110101111000001011110100110010010010000000') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cover_text = tokenizer.decode(sampling_output[0])\n",
    "cover_bits = str2bit(cover_text, tokenizer, model, context=amorce, topk=30)\n",
    "print(\"Bits encodés par le texte stego: {} \\n\".format(cover_bits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(stegotexte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
