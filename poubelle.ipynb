{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import bitarray\n",
    "#from pytorch_transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#from transformers import CamembertLMHeadModel, CamembertTokenizer, CamembertForCausalLM, AutoTokenizer\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_secret = \"I am hidden\"\n",
    "message_secret = message_secret + '<eos>'\n",
    "\n",
    "amorce = \"I am going on a vacation to Italy. I am hoping that my\"\n",
    "\n",
    "model_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_past(past):\n",
    "    past = list(past)\n",
    "    for i in range(len(past)):\n",
    "        past[i] = past[i][:, :, :, -1022:]\n",
    "    return past\n",
    "\n",
    "def int2bits(inp, num_bits):\n",
    "    if num_bits == 0:\n",
    "        return []\n",
    "    strlist = ('{0:0%db}' % num_bits).format(inp)\n",
    "    return [int(strval) for strval in reversed(strlist)]\n",
    "\n",
    "def num_same_from_beg(bits1, bits2):\n",
    "    assert len(bits1) == len(bits2)\n",
    "    for i in range(len(bits1)):\n",
    "        if bits1[i] != bits2[i]:\n",
    "            break\n",
    "\n",
    "    return i\n",
    "\n",
    "def bits2int(bits):\n",
    "    res = 0\n",
    "    for i, bit in enumerate(bits):\n",
    "        res += bit * (2 ** i)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#tokenizer = MeteorTokenizer.from_pretrained(model_name)\n",
    "#tokenizer.unk_token = None\n",
    "#tokenizer.bos_token = None\n",
    "#tokenizer.eos_token = None\n",
    "\n",
    "#model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going on a vacation to Italy. I am hoping that my\n",
      "[40, 716, 1016, 319, 257, 14600, 284, 8031, 13, 314, 716, 7725, 326, 616]\n",
      "I am hidden<eos>\n",
      "[40, 716, 7104, 27, 68, 418, 29]\n"
     ]
    }
   ],
   "source": [
    "encoded_message = tokenizer.encode(message_secret)\n",
    "encoded_context = tokenizer.encode(amorce)\n",
    "\n",
    "print(amorce)\n",
    "print(encoded_context)\n",
    "\n",
    "print(message_secret)\n",
    "print(encoded_message)\n",
    "\n",
    "tensor_amorce = torch.LongTensor(encoded_context).view(1,-1)\n",
    "tensor_message = torch.LongTensor(encoded_message).view(1,-1)\n",
    "#liste des tokens visibles sur:\n",
    "#https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 716, 1016, 319, 257, 14600, 284, 8031, 13, 314, 716, 7725, 326, 616, 3656, 481, 307, 1498, 284, 1282, 736, 2582, 13, 314, 716, 1016, 284, 307, 736]\n",
      "I am going on a vacation to Italy. I am hoping that my wife will be able to come back soon. I am going to be back\n"
     ]
    }
   ],
   "source": [
    "tokens_temp = encoded_context.copy()\n",
    "tensor_tokens = torch.LongTensor(tokens_temp).view(1,-1)\n",
    "for i in range(15):\n",
    "    with torch.no_grad():\n",
    "            outputs = model(tensor_tokens)\n",
    "            logits, past = outputs.logits, outputs.past_key_values\n",
    "            \n",
    "            tokens_temp.append(logits[0,-1].argsort()[-1].item())\n",
    "            tensor_tokens = torch.LongTensor(tokens_temp).view(1,-1)\n",
    "print(tokens_temp)\n",
    "print(tokenizer.decode(tokens_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 716, 1016, 319, 257, 14600, 284, 8031, 13, 314, 716, 7725, 326, 616, 4957, 743, 307, 4642, 290, 4376, 612, 526, 366, 1026, 338, 655, 1165, 4923, 13, 383, 3514, 329, 262, 1641]\n",
      "I am going on a vacation to Italy. I am hoping that my daughter may be born and raised there.\" \"It's just too dangerous. The danger for the family\n"
     ]
    }
   ],
   "source": [
    "tokens_temp = encoded_context.copy()\n",
    "tensor_tokens = torch.LongTensor(tokens_temp).view(1,-1)\n",
    "\n",
    "for i in range(20):\n",
    "    with torch.no_grad():\n",
    "            outputs = model(tensor_tokens)\n",
    "            logits, past = outputs.logits, outputs.past_key_values\n",
    "            \n",
    "            tokens_temp.append(logits[0,-1].argsort()[-round(random.random()*10)-1].item())\n",
    "            tensor_tokens = torch.LongTensor(tokens_temp).view(1,-1)\n",
    "print(tokens_temp)\n",
    "print(tokenizer.decode(tokens_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am hidden<eos>\n",
      "[50256]\n",
      "[40, 716, 7104, 27, 68, 418, 29]\n",
      "tensor([50256])\n",
      "9.094947017729282e-13\n",
      "[0, 0, 1, 0]\n",
      "tensor([40])\n",
      "3.0938210371382067e-12\n",
      "[0, 1]\n",
      "tensor([716])\n",
      "1.6333561215934485e-11\n",
      "[1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1]\n",
      "tensor([7104])\n",
      "3.638922741518681e-12\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0]\n",
      "tensor([27])\n",
      "2.6092516876836965e-12\n",
      "[1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "tensor([68])\n",
      "5.047412821460467e-12\n",
      "[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "tensor([418])\n",
      "1.2635066753028321e-12\n",
      "[0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "encoded message:  bitarray('0010011000110111001011101010111111110100111101100100000010000010000000011110011001111100011100010000000000000')\n"
     ]
    }
   ],
   "source": [
    "class ArithmeticEncoder:\n",
    "    \n",
    "\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.tokenizer   = kwargs.get(\"tokenizer\")\n",
    "        self.precision   = kwargs.get(\"precision\", 40)\n",
    "        self.temperature = kwargs.get(\"temperature\", 1)\n",
    "        self.seed        = kwargs.get(\"seed\", None)\n",
    "        self.topk        = kwargs.get(\"topk\", 60000)\n",
    "        self.model       = kwargs.get(\"model\")\n",
    "        \n",
    "        self.g         = torch.Generator(device=\"cpu\")\n",
    "        if self.seed is None:\n",
    "            self.g.seed()\n",
    "        else:\n",
    "            self.g.manual_seed(self.seed)\n",
    "        pass\n",
    "    \n",
    "    def limit_past(self, past):\n",
    "        past = list(past)\n",
    "        for i in range(len(past)):\n",
    "            past[i] = past[i][:, :, :, -1022:]\n",
    "        return past\n",
    "    \n",
    "    def step(self, current, past):\n",
    "        outputs = self.model(current.unsqueeze(0), past_key_values=past)\n",
    "        logits, past_keys = outputs.logits, outputs.past_key_values\n",
    "        logits[0, -1, -1] = -1e20  # endoftext token can't happen\n",
    "        logits[0, -1, 628] = -1e20  # 2 newlines token can't happen\n",
    "        logits, indices = logits[0, -1, :].sort(descending=True)\n",
    "        logits = logits.double()\n",
    "        logits_temp = logits / self.temperature\n",
    "        probs_temp = F.softmax(logits_temp, dim=0)\n",
    "        \n",
    "        return(probs_temp, past_keys, indices)\n",
    "\n",
    "    def encode(self, message, context=None):\n",
    "\n",
    "        if context is None:\n",
    "            context = self.tokenizer.encode('<|endoftext|>')\n",
    "        print(context)\n",
    "\n",
    "        encoded_message = self.tokenizer.encode(message)\n",
    "        context = torch.tensor(context, dtype=torch.long)\n",
    "        # common BPE error case: 128, 128 (2 newlines) is interpretted as 628 (2 newlines)\n",
    "        i = 0\n",
    "        print(encoded_message)\n",
    "        \n",
    "        while i < len(encoded_message):\n",
    "            if encoded_message[i] == 628:\n",
    "                encoded_message[i] = 198\n",
    "                encoded_message[i + 1:i + 1] = [198]\n",
    "                i += 2\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        max_val = 2 ** self.precision\n",
    "        threshold = 2 ** (-self.precision)\n",
    "        cur_interval = [0, max_val]  # bottom inclusive, top exclusive\n",
    "\n",
    "        prev = context\n",
    "        past_keys = None\n",
    "        bit_message = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            i = 0\n",
    "            while i < len(encoded_message):\n",
    "                print(prev)\n",
    "                probs_temp, past_keys, indices = self.step(prev, past_keys)\n",
    "                cum_probs = probs_temp.cumsum(0)\n",
    "\n",
    "                # Cutoff low probabilities that would be rounded to 0\n",
    "                cur_int_range = cur_interval[1] - cur_interval[0]\n",
    "                cur_threshold = 1 / cur_int_range\n",
    "                print(cur_threshold)\n",
    "                \n",
    "                k = min(max(2, (probs_temp < cur_threshold).nonzero()[0].item()), self.topk)\n",
    "                probs_temp_int = probs_temp[:k]  # Cutoff all but top k\n",
    "                probs_temp_int = probs_temp_int / probs_temp_int.sum()  * cur_int_range\n",
    "\n",
    "                # Round probabilities to integers given precision\n",
    "                probs_temp_int = probs_temp_int.round().long()\n",
    "                cum_probs = probs_temp_int.cumsum(0)\n",
    "\n",
    "                # Remove any elements from the bottom if rounding caused the total prob to be too large\n",
    "                overfill_index = (cum_probs > cur_int_range).nonzero()\n",
    "                if len(overfill_index) > 0:\n",
    "                    cum_probs = cum_probs[:overfill_index[0]]\n",
    "                    k = overfill_index[0].item()\n",
    "\n",
    "                # Add any mass to the top if removing/rounding causes the total prob to be too small\n",
    "                cum_probs += cur_int_range - cum_probs[-1]  # add\n",
    "\n",
    "                # Covnert to position in range\n",
    "                cum_probs += cur_interval[0]\n",
    "\n",
    "                rank = (indices == encoded_message[i]).nonzero().item()\n",
    "                \n",
    "                if rank >= k:\n",
    "                    true_token_text = enc.decoder[inp[i]]\n",
    "                    for rank_idx in range(k):\n",
    "                        prop_token_text = enc.decoder[indices[rank_idx].item()]\n",
    "                        # common case that is not caught\n",
    "                        if inp[i] == 128 and indices[rank_idx] == 198:\n",
    "                            rank = rank_idx\n",
    "                            inp[i] = indices[rank_idx].item()\n",
    "                            break\n",
    "\n",
    "                        # Is there a more likely prefix token that could be the actual token generated?\n",
    "                        if len(prop_token_text) <= len(true_token_text) and \\\n",
    "                                prop_token_text == true_token_text[:len(prop_token_text)]:\n",
    "                            rank = rank_idx\n",
    "                            suffix = true_token_text[len(prop_token_text):]\n",
    "                            suffix_tokens = enc.encode(suffix)  # a list\n",
    "                            inp[i] = indices[rank_idx].item()\n",
    "                            inp[i + 1:i + 1] = suffix_tokens  # insert suffix tokens into list\n",
    "                            break\n",
    "\n",
    "                        # Is there a more likely longer token that could be the actual token generated?\n",
    "                        elif len(prop_token_text) > len(true_token_text) and \\\n",
    "                                true_token_text == prop_token_text[:len(true_token_text)]:\n",
    "                            whole_text = true_token_text\n",
    "                            num_extra = 1\n",
    "                            while len(whole_text) < len(prop_token_text):\n",
    "                                whole_text += enc.decoder[inp[i + num_extra]]\n",
    "                                num_extra += 1\n",
    "                            if prop_token_text == whole_text[:len(prop_token_text)]:\n",
    "                                rank = rank_idx\n",
    "                                inp[i] = indices[rank_idx].item()\n",
    "                                for j in range(1, num_extra):\n",
    "                                    del inp[i + j]\n",
    "\n",
    "                                if len(whole_text) > len(prop_token_text):\n",
    "                                    suffix = whole_text[len(prop_token_text):]\n",
    "                                    suffix_tokens = enc.encode(suffix)  # a list\n",
    "                                    inp[i + 1:i + 1] = suffix_tokens  # insert suffix tokens into list\n",
    "                                break\n",
    "                    else:\n",
    "                        print('Unable to fix BPE error: token received: %s=%d, text: %s' % (true_token_text, inp[i], text))\n",
    "                        rank = 0\n",
    "                    \n",
    "                selection = rank\n",
    "\n",
    "                # Calculate new range as ints\n",
    "                new_int_bottom = cum_probs[selection - 1] if selection > 0 else cur_interval[0]\n",
    "                new_int_top = cum_probs[selection]\n",
    "\n",
    "                # Convert range to bits\n",
    "                new_int_bottom_bits_inc = list(reversed(int2bits(new_int_bottom, self.precision)))\n",
    "                new_int_top_bits_inc = list(\n",
    "                    reversed(int2bits(new_int_top - 1, self.precision)))  # -1 here because upper bound is exclusive\n",
    "\n",
    "                # Emit most significant bits which are now fixed and update interval\n",
    "                num_bits_encoded = num_same_from_beg(new_int_bottom_bits_inc, new_int_top_bits_inc)\n",
    "                if i == len(encoded_message) - 1:\n",
    "                    new_bits = new_int_bottom_bits_inc\n",
    "                else:\n",
    "                    new_bits = new_int_top_bits_inc[:num_bits_encoded]\n",
    "                bit_message += new_bits\n",
    "                print(new_bits)\n",
    "\n",
    "                new_int_bottom_bits = new_int_bottom_bits_inc[num_bits_encoded:] + [0] * num_bits_encoded\n",
    "                new_int_top_bits = new_int_top_bits_inc[num_bits_encoded:] + [1] * num_bits_encoded\n",
    "\n",
    "                cur_interval[0] = bits2int(reversed(new_int_bottom_bits))\n",
    "                cur_interval[1] = bits2int(reversed(new_int_top_bits)) + 1  # +1 here because upper bound is exclusive\n",
    " \n",
    "                # Update history with new token\n",
    "                prev = torch.tensor([encoded_message[i]], dtype=torch.long)\n",
    "                i += 1\n",
    "\n",
    "        return bit_message\n",
    "\n",
    "print(message_secret)\n",
    "encoder = ArithmeticEncoder(model=model, tokenizer=tokenizer)\n",
    "bit_message = bitarray.bitarray(encoder.encode(message_secret))\n",
    "print(\"encoded message: \", bit_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah = np.array([[0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "eh = [0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 108\n"
     ]
    }
   ],
   "source": [
    "print(len(bit_message), len(eh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "z est reparti :  tensor([0.0625, 0.1003, 0.1245, 0.1439, 0.1623, 0.1739, 0.1852, 0.1944, 0.2031,\n",
      "        0.2109], dtype=torch.float64) 0 1099511627776\n",
      "tensor([198, 464,   1,  32,  40])\n",
      "==========================================\n",
      "==========================================\n",
      "4 I\n",
      "z est reparti :  tensor([0.0934, 0.1815, 0.2446, 0.2920, 0.3389, 0.3677, 0.3938, 0.4166, 0.4380,\n",
      "        0.4543], dtype=torch.float64) 332922693904 656147595408\n",
      "tensor([1101, 1053,  423,  716,  373])\n",
      "==========================================\n",
      "==========================================\n",
      "3 I am\n",
      "z est reparti :  tensor([0.1394, 0.2198, 0.2520, 0.2804, 0.3054, 0.3279, 0.3497, 0.3712, 0.3910,\n",
      "        0.4082], dtype=torch.float64) 548452273768 609675909364\n",
      "tensor([ 257,  407, 1016,  281,  523])\n",
      "==========================================\n",
      "==========================================\n",
      "7330 I am hidden\n",
      "z est reparti :  tensor([0.2105, 0.3517, 0.4276, 0.4980, 0.5351, 0.5698, 0.6009, 0.6282, 0.6550,\n",
      "        0.6795], dtype=torch.float64) 279195942912 554002546688\n",
      "tensor([ 287,  422, 2157, 1497,  416])\n",
      "==========================================\n",
      "==========================================\n",
      "2843 I am hidden xx\n",
      "z est reparti :  tensor([0.1551, 0.2144, 0.2455, 0.2692, 0.2863, 0.3033, 0.3177, 0.3299, 0.3407,\n",
      "        0.3507], dtype=torch.float64) 530565562368 892823404544\n",
      "tensor([  198, 31383,    13,    12,    87])\n",
      "==========================================\n",
      "==========================================\n",
      "2327 I am hidden xx SH\n",
      "z est reparti :  tensor([0.0647, 0.1142, 0.1585, 0.2012, 0.2384, 0.2704, 0.2942, 0.3161, 0.3359,\n",
      "        0.3549], dtype=torch.float64) 547258273792 565883586560\n",
      "tensor([   46,  4061,  1268,  3843, 12203])\n",
      "==========================================\n",
      "==========================================\n",
      "36 I am hidden xx SHE\n",
      "z est reparti :  tensor([0.0454, 0.0835, 0.1068, 0.1279, 0.1470, 0.1636, 0.1775, 0.1912, 0.2037,\n",
      "        0.2159], dtype=torch.float64) 138958045184 962117476352\n",
      "tensor([ 318,  198,  468, 3069,  509])\n",
      "==========================================\n",
      "==========================================\n",
      "5 I am hidden xx SHEK\n",
      "z est reparti :  tensor([0.0283, 0.0505, 0.0717, 0.0927, 0.1119, 0.1260, 0.1383, 0.1493, 0.1603,\n",
      "        0.1712], dtype=torch.float64) 147127731264 1019986225280\n",
      "tensor([ 198, 1546, 1268,   42,   40])\n",
      "==========================================\n",
      "==========================================\n",
      "3 I am hidden xx SHEKK\n",
      "z est reparti :  tensor([0.0395, 0.0586, 0.0758, 0.0923, 0.1064, 0.1201, 0.1324, 0.1443, 0.1553,\n",
      "        0.1661], dtype=torch.float64) 112938790304 699228677120\n",
      "tensor([198,  42,  40,   0,  56])\n",
      "==========================================\n",
      "==========================================\n",
      "371 I am hidden xx SHEKK,\n",
      "z est reparti :  tensor([0.0499, 0.0875, 0.1123, 0.1310, 0.1489, 0.1611, 0.1728, 0.1842, 0.1954,\n",
      "        0.2039], dtype=torch.float64) 167371788288 593216776192\n",
      "tensor([314, 290, 198, 345, 262])\n",
      "==========================================\n",
      "==========================================\n",
      "19042 I am hidden xx SHEKK, Barn\n",
      "z est reparti :  tensor([0.1200, 0.2346, 0.2895, 0.3396, 0.3858, 0.4227, 0.4458, 0.4686, 0.4907,\n",
      "        0.5062], dtype=torch.float64) 549742219840 549758239680\n",
      "tensor([3930,  446, 6008,  388,  397])\n",
      "==========================================\n",
      "==========================================\n",
      "733 I am hidden xx SHEKK, Barncamp\n",
      "z est reparti :  tensor([0.0921, 0.1515, 0.1982, 0.2237, 0.2428, 0.2604, 0.2729, 0.2852, 0.2955,\n",
      "        0.3044], dtype=torch.float64) 549755813559 549755814902\n",
      "tensor([ 11, 837, 198,  13, 290])\n",
      "==========================================\n",
      "==========================================\n",
      "0 I am hidden xx SHEKK, Barncamp,\n",
      "z est reparti :  tensor([0.0386, 0.0752, 0.1031, 0.1182, 0.1319, 0.1455, 0.1584, 0.1711, 0.1831,\n",
      "        0.1945], dtype=torch.float64) 549755813559 549755814185\n",
      "tensor([ 7257,  6645,  3482, 15326, 18732])\n",
      "==========================================\n",
      "==========================================\n",
      "10 I am hidden xx SHEKK, Barncamp, USA\n",
      "z est reparti :  tensor([0.2823, 0.3652, 0.4241, 0.4472, 0.4696, 0.4854, 0.4984, 0.5093, 0.5178,\n",
      "        0.5248], dtype=torch.float64) 549755813883 549755813890\n",
      "tensor([198,  13,  11, 357, 314])\n",
      "==========================================\n",
      "==========================================\n",
      "0 I am hidden xx SHEKK, Barncamp, USA\n",
      "\n",
      "z est reparti :  tensor([0.9348, 0.9479, 0.9502, 0.9521, 0.9537, 0.9552, 0.9565, 0.9573, 0.9580,\n",
      "        0.9588], dtype=torch.float64) 549755813883 549755813889\n",
      "tensor([ 198,   40,  464, 3666, 1212])\n",
      "==========================================\n",
      "==========================================\n",
      "0 I am hidden xx SHEKK, Barncamp, USA\n",
      "\n",
      "\n",
      "z est reparti :  tensor([0.1704, 0.1972, 0.2100, 0.2221, 0.2337, 0.2449, 0.2548, 0.2647, 0.2745,\n",
      "        0.2842], dtype=torch.float64) 549755813883 549755813889\n",
      "tensor([   40,  3666,   464, 14231,  1212])\n",
      "==========================================\n",
      "==========================================\n",
      "0 I am hidden xx SHEKK, Barncamp, USA\n",
      "\n",
      "I\n",
      "z est reparti :  tensor([0.5073, 0.6007, 0.6578, 0.7049, 0.7190, 0.7298, 0.7392, 0.7481, 0.7565,\n",
      "        0.7645], dtype=torch.float64) 549755813883 549755813889\n",
      "tensor([ 716, 1101,  423,  373, 1842])\n",
      "==========================================\n",
      "==========================================\n",
      "1 I am hidden xx SHEKK, Barncamp, USA\n",
      "\n",
      "I'm\n",
      "I am hidden<eos>\n",
      "[40, 716, 7104, 27, 68, 418, 29]\n",
      "decoded message:  [40, 716, 7104, 31383, 6006, 36, 42, 42, 837, 11842, 16544, 11, 4916, 198, 198, 40, 1101]\n",
      "I am hidden xx SHEKK, Barncamp, USA\n",
      "\n",
      "I'm\n"
     ]
    }
   ],
   "source": [
    "class ArithmeticDecoder:\n",
    "    \n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.tokenizer   = kwargs.get(\"tokenizer\")\n",
    "        self.precision   = kwargs.get(\"precision\", 40)\n",
    "        self.temperature = kwargs.get(\"temperature\", 1)\n",
    "        self.seed        = kwargs.get(\"seed\", None)\n",
    "        self.topk        = kwargs.get(\"topk\", 60000)\n",
    "        self.model       = kwargs.get(\"model\")\n",
    "        \n",
    "        self.g         = torch.Generator(device=\"cpu\")\n",
    "        if self.seed is None:\n",
    "            self.g.seed()\n",
    "        else:\n",
    "            self.g.manual_seed(self.seed)\n",
    "        pass\n",
    "    \n",
    "    def limit_past(self, past):\n",
    "        past = list(past)\n",
    "        for i in range(len(past)):\n",
    "            past[i] = past[i][:, :, :, -1022:]\n",
    "        return past\n",
    "    \n",
    "    def step(self, current, past):\n",
    "        outputs = self.model(current.unsqueeze(0), past_key_values=past)\n",
    "        logits, past_keys = outputs.logits, outputs.past_key_values\n",
    "        logits[0, -1, -1] = -1e20  # endoftext token can't happen\n",
    "        logits[0, -1, 628] = -1e20  # 2 newlines token can't happen\n",
    "        logits, indices = logits[0, -1, :].sort(descending=True)\n",
    "        logits = logits.double()\n",
    "        logits_temp = logits / self.temperature\n",
    "        probs_temp = F.softmax(logits_temp, dim=0)\n",
    "        \n",
    "        return(probs_temp, past_keys, indices)\n",
    "    \n",
    "    def decode(self, message, context=None, finish_sent=False):\n",
    "        if context is None:\n",
    "            context = self.tokenizer.encode('<|endoftext|>')\n",
    "            \n",
    "        max_val = 2 ** self.precision\n",
    "        threshold = 2 ** (- self.precision)\n",
    "        cur_interval = [0, max_val]  # bottom inclusive, top exclusive\n",
    "            \n",
    "        context = torch.tensor(context, dtype=torch.long)\n",
    "\n",
    "        prev = context\n",
    "        output = context\n",
    "        past_keys = None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            i = 0\n",
    "            sent_finish = False\n",
    "            while i < len(message) or (finish_sent and not sent_finish):\n",
    "                probs, past_keys, indices = self.step(prev, past_keys)\n",
    "                print('z est reparti : ', probs.cumsum(0)[:10], cur_interval[0], cur_interval[1])\n",
    "\n",
    "                # conditions for having reached the end of the message\n",
    "                if i >= len(message):\n",
    "                    selection = 0\n",
    "                    sent_finish = is_sent_finish(indices[selection].item(), enc)\n",
    "                else:\n",
    "                    # Cutoff low probabilities that would be rounded to 0\n",
    "                    cur_int_range = cur_interval[1] - cur_interval[0]\n",
    "                    cur_threshold = 1 / cur_int_range    \n",
    "                    k = min(max(2, (probs < cur_threshold).nonzero()[0].item()), self.topk)\n",
    "                    probs_temp_int = probs[:k]  # Cutoff all but top k\n",
    "\n",
    "                    # Rescale to correct range\n",
    "                    probs_temp_int = probs/probs.sum() * cur_int_range\n",
    "\n",
    "                    # Round probabilities to integers given precision\n",
    "                    probs_temp_int = probs_temp_int.round().long()\n",
    "                    cum_probs = probs_temp_int.cumsum(0)\n",
    "\n",
    "                    # Remove any elements from the bottom if rounding caused the total prob to be too large\n",
    "                    overfill_index = (cum_probs > cur_int_range).nonzero()\n",
    "                    if len(overfill_index) > 0:\n",
    "                        cum_probs = cum_probs[:overfill_index[0]]\n",
    "\n",
    "                    # Add any mass to the top if removing/rounding causes the total prob to be too small\n",
    "                    cum_probs += cur_int_range - cum_probs[-1]  # add\n",
    "\n",
    "                    # Get out resulting probabilities\n",
    "                    probs_final = cum_probs.clone()\n",
    "                    probs_final[1:] = cum_probs[1:] - cum_probs[:-1]\n",
    "\n",
    "                    # Convert to position in range\n",
    "                    cum_probs += cur_interval[0]\n",
    "\n",
    "                    # Get selected index based on binary fraction from message bits\n",
    "                    message_bits = message[i:i + self.precision]\n",
    "                    if i + self.precision > len(message):\n",
    "                        message_bits = message_bits + [0] * (i + self.precision - len(message))\n",
    "                    message_idx = bits2int(reversed(message_bits))\n",
    "                    selection = (cum_probs > message_idx).nonzero()[0].item()\n",
    "         \n",
    "                    # Calculate new range as ints\n",
    "                    new_int_bottom = cum_probs[selection - 1] if selection > 0 else cur_interval[0]\n",
    "                    new_int_top = cum_probs[selection]\n",
    "\n",
    "                    # Convert range to bits\n",
    "                    new_int_bottom_bits_inc = list(reversed(int2bits(new_int_bottom, self.precision)))\n",
    "                    new_int_top_bits_inc = list(\n",
    "                        reversed(int2bits(new_int_top - 1, self.precision)))  # -1 here because upper bound is exclusive\n",
    "                    # Consume most significant bits which are now fixed and update interval\n",
    "                    num_bits_encoded = num_same_from_beg(new_int_bottom_bits_inc, new_int_top_bits_inc)\n",
    "                    i += num_bits_encoded\n",
    "\n",
    "                    new_int_bottom_bits = new_int_bottom_bits_inc[num_bits_encoded:] + [0] * num_bits_encoded\n",
    "                    new_int_top_bits = new_int_top_bits_inc[num_bits_encoded:] + [1] * num_bits_encoded\n",
    "\n",
    "                    cur_interval[0] = bits2int(reversed(new_int_bottom_bits))\n",
    "                    cur_interval[1] = bits2int(reversed(new_int_top_bits)) + 1  # +1 here because upper bound is exclusive\n",
    "\n",
    "\n",
    "                # Update history with new token\n",
    "                prev = indices[selection].view(1)\n",
    "                output = torch.cat((output, prev))\n",
    "\n",
    "                # For text->bits->text\n",
    "                partial = self.tokenizer.decode(output[len(context):].tolist())\n",
    "                print(indices[:5])\n",
    "                print(\"==========================================\")\n",
    "                print(\"==========================================\")\n",
    "                if '<eos>' in partial:\n",
    "                    break\n",
    "                \n",
    "                print(selection, partial)\n",
    "        return output[len(context):].tolist()\n",
    "\n",
    "    \n",
    "decoder = ArithmeticDecoder(model=model, tokenizer=tokenizer, topk=60000)\n",
    "print(eh[0])\n",
    "bit_message_2 = bitarray.bitarray(eh)\n",
    "decoded_message = decoder.decode(bit_message)\n",
    "\n",
    "\n",
    "print(message_secret)\n",
    "print(encoded_message)\n",
    "print(\"decoded message: \",decoded_message)\n",
    "print(tokenizer.decode(decoded_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z est reparti :  tensor([0.0824, 0.1287, 0.1568, 0.1798, 0.2009, 0.2190, 0.2364, 0.2536, 0.2696,\n",
      "        0.2825], dtype=torch.float64) 0 1099511627776\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a3f5e15804b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArithmeticDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdecoded_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoded_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decoded message: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoded_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-0fac0c44cfa9>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, message, context, finish_sent)\u001b[0m\n\u001b[1;32m     97\u001b[0m                         \u001b[0mmessage_bits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage_bits\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0mmessage_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbits2int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_bits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                     \u001b[0mselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcum_probs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmessage_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0;31m# Calculate new range as ints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "decoder = ArithmeticDecoder(model=model, tokenizer=tokenizer, topk=60000)\n",
    "decoded_message = decoder.decode(encoded_message, context=encoded_context)\n",
    "print(\"decoded message: \",decoded_message)\n",
    "print(tokenizer.decode(decoded_message))\n",
    "\n",
    "encoder = ArithmeticEncoder(model=model, tokenizer=tokenizer, topk=60000)\n",
    "encoded_message = encoder.encode(message_secret, contexte=encoded_context)\n",
    "print(\"encoded message: \",encoded_message)\n",
    "\n",
    "decoder = ArithmeticDecoder(model=model, tokenizer=tokenizer, topk=60000)\n",
    "decoded_message = decoder.decode(encoded_message)\n",
    "print(\"decoded message: \",decoded_message)\n",
    "print(tokenizer.decode(decoded_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(amorce, message_secret)\n",
    "#bit2str(encoded_message, model, tokenizer)[:-5]\n",
    "#str2bits(model=model, enc=tokenizer, message=message_secret)\n",
    "#decoded_message = bit2str(encoded_message, model, tokenizer)[:-5]\n",
    "encoded_message = str2bits(model=model, enc=tokenizer, message=message_secret)\n",
    "print(encoded_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sent_finish(token_idx, enc):\n",
    "    token = enc.decode(token_idx)\n",
    "    print(token)\n",
    "    return '.' in token or '!' in token or '?' in token\n",
    "\n",
    "def StegaGen(method, tokenizer, model, proc_seed): \n",
    "    \n",
    "    if method == 'arithmetic':\n",
    "        encoder_decoder = ArithmeticStega(tokenizer=tokenizer, model=model, seed=proc_seed) \n",
    "        \n",
    "    elif method == 'adg':\n",
    "        encoder_decoder = ArithmeticStega(tokenizer=tokenizer,model=model, seed=proc_seed)   \n",
    "    else:\n",
    "        print('Unkown method: ', method)\n",
    "        \n",
    "    return(encoder_decoder)\n",
    "\n",
    "\n",
    "class ArithmetiEncoder:\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.precision = kwargs.get(\"precision\", 32)\n",
    "        self.seed      = kwargs.get(\"seed\", None)\n",
    "        self.topk      = kwargs.get(\"topk\", 10)\n",
    "        self.topp      = kwargs.get(\"topp\", None)\n",
    "        self.model     = kwargs.get(\"model\")\n",
    "        \n",
    "        self.g         = torch.Generator(device=\"cpu\")\n",
    "        if self.seed is None:\n",
    "            self.g.seed()\n",
    "        else:\n",
    "            self.g.manual_seed(self.seed)\n",
    "        pass\n",
    "    \n",
    "    def reset(self, context):\n",
    "        return self.step(context, None)\n",
    "    \n",
    "    def reajust_probs(self, logits, indices):\n",
    "        probs = F.softmax(logits , dim=0)\n",
    "        if self.topp is not None:\n",
    "            overfill_index = ((probs.cumsum(0) > self.topp).nonzero())[0]\n",
    "            reajusted_probs = probs[:overfill_index]\n",
    "            reajusted_indices = indices[:overfill_index]\n",
    "        elif self.topk is not None:\n",
    "            reajusted_probs = probs[:self.topk]\n",
    "            reajusted_indices = indices[:self.topk]\n",
    "        return(F.softmax(reajusted_probs , dim=0), reajusted_indices)\n",
    "    \n",
    "    def step(self, current, past):\n",
    "        outputs = self.model(current.unsqueeze(0), past_key_values=past)\n",
    "        logits, past_keys = outputs.logits, outputs.past_key_values\n",
    "        \n",
    "        logits[0, -1, -1] = -1e20  # endoftext token can't happen\n",
    "        logits[0, -1, 628] = -1e20  # 2 newlines token can't happen\n",
    "        logits, indices = logits[0, -1, :].sort(descending=True)\n",
    "        \n",
    "        probs, indices = self.reajust_probs(logits, indices)\n",
    "        \n",
    "        return(probs, past_keys, indices)\n",
    "        \n",
    "    def encode(self, private_message_bit: bitarray.bitarray, context: str = None):\n",
    "        \"\"\"\n",
    "        :param msg: np array of 0s and 1s constituting a message bitstring\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        message = private_message_bit\n",
    "        tokenizer = self.tokenizer\n",
    "        precision = self.precision\n",
    "\n",
    "        max_val = 2 ** precision\n",
    "        cur_interval = [0, max_val]  # bottom inclusive, top exclusive\n",
    "\n",
    "        prev = context\n",
    "        print(\"context is \", context)\n",
    "        enc_prev = torch.LongTensor(tokenizer.encode(context))\n",
    "        \n",
    "        _, _, _ = self.reset(enc_prev)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            i = 0\n",
    "            j = 0\n",
    "            past=None\n",
    "            sent_finish = False\n",
    "            while i < len(message) or (not sent_finish):\n",
    "\n",
    "                probs, past, indices = self.step(enc_prev, past)\n",
    "                          \n",
    "                j += 1\n",
    "                probs_temp = probs\n",
    "\n",
    "                # conditions for having reached the end of the message\n",
    "                if i >= len(message):\n",
    "                    token = tokenizer.decode(token_idx)\n",
    "                    sent_finish = '.' in token or '!' in token or '?' in token\n",
    "                    print(\"token: \", token,\"sent_finish: \",  sent_finish)\n",
    "                else:\n",
    "                    # Cutoff low probabilities that would be rounded to 0\n",
    "                    cur_int_range = cur_interval[1] - cur_interval[0]\n",
    "\n",
    "                    # Rescale to correct range\n",
    "                    probs_temp_int = probs_temp * cur_int_range\n",
    "\n",
    "                    # Round probabilities to integers given precision\n",
    "                    probs_temp_int = probs_temp_int.round().long()\n",
    "                    cum_probs = probs_temp_int.cumsum(0)\n",
    "                    \n",
    "                    # Remove any elements from the bottom if rounding caused the total prob to be too large\n",
    "                    overfill_index = (cum_probs > cur_int_range).nonzero()\n",
    "                    print(overfill_index, \"uh\")\n",
    "                    if len(overfill_index) > 0:\n",
    "                        cum_probs = cum_probs[:overfill_index[0]]\n",
    "\n",
    "                    print(cum_probs[-1], \"ah\")\n",
    "                    # Add any mass to the top if removing/rounding causes the total prob to be too small\n",
    "                    cum_probs += cur_int_range - cum_probs[-1]  # add\n",
    "\n",
    "                    # Get out resulting probabilities\n",
    "                    probs_final = cum_probs.clone()\n",
    "                    probs_final[1:] = cum_probs[1:] - cum_probs[:-1]\n",
    "\n",
    "                    # Convert to position in range\n",
    "                    cum_probs += cur_interval[0]\n",
    "\n",
    "                    # Get selected index based on binary fraction from message bits\n",
    "                    message_bits = message[i:i + precision]\n",
    "                    if i + precision > len(message):\n",
    "                        message_bits = message_bits + [0] * (i + precision - len(message))\n",
    "                    message_idx = bits2int(reversed(message_bits))\n",
    "                    selection = (cum_probs > message_idx).nonzero()[0].item()\n",
    "                    \n",
    "                    # Calculate new range as ints\n",
    "                    new_int_bottom = cum_probs[selection - 1] if selection > 0 else cur_interval[0]\n",
    "                    new_int_top = cum_probs[selection]\n",
    "\n",
    "                    # Convert range to bits\n",
    "                    new_int_bottom_bits_inc = list(reversed(int2bits(new_int_bottom, precision)))\n",
    "                    new_int_top_bits_inc = list(reversed(int2bits(new_int_top - 1, precision)))  \n",
    "                    # -1 here because upper bound is exclusive\n",
    "\n",
    "                    # Consume most significant bits which are now fixed and update interval\n",
    "                    num_bits_encoded = num_same_from_beg(new_int_bottom_bits_inc, new_int_top_bits_inc)\n",
    "                    i += num_bits_encoded\n",
    "\n",
    "                    new_int_bottom_bits = new_int_bottom_bits_inc[num_bits_encoded:] + [0] * num_bits_encoded\n",
    "                    new_int_top_bits = new_int_top_bits_inc[num_bits_encoded:] + [1] * num_bits_encoded\n",
    "\n",
    "                    cur_interval[0] = bits2int(reversed(new_int_bottom_bits))\n",
    "                    cur_interval[1] = bits2int(reversed(new_int_top_bits)) + 1  \n",
    "                    # +1 here because upper bound is exclusive\n",
    "\n",
    "                    q = probs_final.double() / probs_final.sum()\n",
    "\n",
    "                # Update history with new token\n",
    "                prev = indices[selection].view(1)\n",
    "                \n",
    "                enc_prev = torch.cat((enc_prev, prev))\n",
    "                print(\"current message: \", tokenizer.decode(enc_prev), prev, selection)\n",
    "\n",
    "                # For text->bits->text\n",
    "                partial = self.tokenizer.decode(enc_prev[len(context):].tolist())\n",
    "                if '<eos>' in partial:\n",
    "                    break\n",
    "\n",
    "\n",
    "        return self.medium.enc.decode(enc_prev[len(context):].tolist()), enc_prev[len(context):].tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = -1\n",
    "proc_seed = (seed * 55555) % 77777\n",
    "proc_rng = np.random.default_rng(proc_seed)\n",
    "\n",
    "# Now branch into different methods\n",
    "encoder_decoder = StegaGen('arithmetic', tokenizer, model, proc_seed)\n",
    "\n",
    "print(encoded_message, amorce)\n",
    "ah = model( torch.LongTensor(tokenizer.encode(amorce)).unsqueeze(0)  )\n",
    "logits=  ah.logits\n",
    "print(logits.argmax())\n",
    "\n",
    "# Proceed with communication\n",
    "public_message_str, public_message_token = encoder_decoder.encode(private_message_bit=encoded_message,context=amorce)\n",
    "print(public_message_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def decode(self, public_message_token, text=None, context: str = None, private_message_bitlen=None, **kwargs):\n",
    "\n",
    "        inp = public_message_token\n",
    "        enc = self.medium.enc\n",
    "        device = self.medium.device  # may want to restrict to CPU!\n",
    "        precision = self.precision\n",
    "        max_val = 2 ** precision\n",
    "\n",
    "        max_val = 2 ** precision\n",
    "        cur_interval = [0, max_val]  # bottom inclusive, top exclusive\n",
    "        \n",
    "        t_iter_1 = None\n",
    "\n",
    "        enc_context = self.medium.encode_context(context)\n",
    "        enc_context = th.tensor(enc_context[-1022:], device=device, dtype=th.long)\n",
    "        prev = enc_context\n",
    "        message = []\n",
    "        with th.no_grad():\n",
    "            i = 0\n",
    "            j = 0\n",
    "            while i < len(inp):\n",
    "\n",
    "                if t_iter_1 is not None:\n",
    "                    delta_t_step_no_medium = time.time() - t_iter_1\n",
    "                t_medium_1 = time.time()\n",
    "\n",
    "                if j == 0:\n",
    "                    probs, info = self.medium.reset(context=context)\n",
    "                else:\n",
    "                    probs, info = self.medium.step(prev)\n",
    "                j += 1\n",
    "                probs = th.from_numpy(probs.astype(np.float64))\n",
    "\n",
    "                delta_t_medium = time.time() - t_medium_1\n",
    "                t_iter_1 = time.time()\n",
    "\n",
    "                probs_temp = probs\n",
    "                indices = th.from_numpy(info[\"indices\"])\n",
    "                probs_temp_int = probs_temp\n",
    "\n",
    "                # Cutoff low probabilities that would be rounded to 0\n",
    "                cur_int_range = cur_interval[1] - cur_interval[0]\n",
    "\n",
    "                # Rescale to correct range\n",
    "                probs_temp_int = probs_temp_int / probs_temp_int.sum() * cur_int_range\n",
    "\n",
    "                # Round probabilities to integers given precision\n",
    "                probs_temp_int = probs_temp_int.round().long()\n",
    "                cum_probs = probs_temp_int.cumsum(0)\n",
    "\n",
    "                # Remove any elements from the bottom if rounding caused the total prob to be too large\n",
    "                overfill_index = (cum_probs > cur_int_range).nonzero()\n",
    "                if len(overfill_index) > 0:\n",
    "                    cum_probs = cum_probs[:overfill_index[0]]\n",
    "                    k = overfill_index[0].item()\n",
    "\n",
    "                # Add any mass to the top if removing/rounding causes the total prob to be too small\n",
    "                cum_probs += cur_int_range - cum_probs[-1]  # add\n",
    "\n",
    "                # Covnert to position in range\n",
    "                cum_probs += cur_interval[0]\n",
    "\n",
    "                rank = (indices == inp[i]).nonzero().item()\n",
    "\n",
    "                k = len(probs)\n",
    "                # Handle most errors that could happen because of BPE with heuristic\n",
    "                if rank >= k:\n",
    "                    true_token_text = enc.decoder[inp[i]]\n",
    "                    for rank_idx in range(k):\n",
    "                        prop_token_text = enc.decoder[indices[rank_idx].item()]\n",
    "                        # common case that is not caught\n",
    "                        if inp[i] == 128 and indices[rank_idx] == 198:\n",
    "                            rank = rank_idx\n",
    "                            inp[i] = indices[rank_idx].item()\n",
    "                            break\n",
    "\n",
    "                        # Is there a more likely prefix token that could be the actual token generated?\n",
    "                        if len(prop_token_text) <= len(true_token_text) and \\\n",
    "                                prop_token_text == true_token_text[:len(prop_token_text)]:\n",
    "                            rank = rank_idx\n",
    "                            suffix = true_token_text[len(prop_token_text):]\n",
    "                            suffix_tokens = enc.encode(suffix)  # a list\n",
    "                            inp[i] = indices[rank_idx].item()\n",
    "                            inp[i + 1:i + 1] = suffix_tokens  # insert suffix tokens into list\n",
    "                            break\n",
    "\n",
    "                        # Is there a more likely longer token that could be the actual token generated?\n",
    "                        elif len(prop_token_text) > len(true_token_text) and \\\n",
    "                                true_token_text == prop_token_text[:len(true_token_text)]:\n",
    "                            whole_text = true_token_text\n",
    "                            num_extra = 1\n",
    "                            while len(whole_text) < len(prop_token_text):\n",
    "                                whole_text += enc.decoder[inp[i + num_extra]]\n",
    "                                num_extra += 1\n",
    "                            if prop_token_text == whole_text[:len(prop_token_text)]:\n",
    "                                rank = rank_idx\n",
    "                                inp[i] = indices[rank_idx].item()\n",
    "                                for j in range(1, num_extra):\n",
    "                                    del inp[i + j]\n",
    "\n",
    "                                if len(whole_text) > len(prop_token_text):\n",
    "                                    suffix = whole_text[len(prop_token_text):]\n",
    "                                    suffix_tokens = enc.encode(suffix)  # a list\n",
    "                                    inp[i + 1:i + 1] = suffix_tokens  # insert suffix tokens into list\n",
    "                                break\n",
    "                    else:\n",
    "                        print('Unable to fix BPE error: token received: %s=%d, text: %s' % (\n",
    "                            true_token_text, inp[i], text))\n",
    "                        rank = 0\n",
    "\n",
    "                selection = rank\n",
    "\n",
    "                # Calculate new range as ints\n",
    "                new_int_bottom = cum_probs[selection - 1] if selection > 0 else cur_interval[0]\n",
    "                new_int_top = cum_probs[selection]\n",
    "\n",
    "                # Convert range to bits\n",
    "                new_int_bottom_bits_inc = list(reversed(int2bits(new_int_bottom, precision)))\n",
    "                new_int_top_bits_inc = list(reversed(int2bits(new_int_top - 1, precision)))  \n",
    "                # -1 because upper bound is exclusive\n",
    "\n",
    "                # Emit most significant bits which are now fixed and update interval\n",
    "                num_bits_encoded = num_same_from_beg(new_int_bottom_bits_inc, new_int_top_bits_inc)\n",
    "                if i == len(inp) - 1:\n",
    "                    new_bits = new_int_bottom_bits_inc\n",
    "                else:\n",
    "                    new_bits = new_int_top_bits_inc[:num_bits_encoded]\n",
    "                message += new_bits\n",
    "\n",
    "                new_int_bottom_bits = new_int_bottom_bits_inc[num_bits_encoded:] + [0] * num_bits_encoded\n",
    "                new_int_top_bits = new_int_top_bits_inc[num_bits_encoded:] + [1] * num_bits_encoded\n",
    "\n",
    "                cur_interval[0] = bits2int(reversed(new_int_bottom_bits))\n",
    "                cur_interval[1] = bits2int(reversed(new_int_top_bits)) + 1  \n",
    "                # +1 because upper bound is exclusive\n",
    "\n",
    "                # Update history with new token\n",
    "                prev = th.tensor([inp[i]], device=device, dtype=th.long)\n",
    "                i += 1\n",
    "\n",
    "        output = bitarray.bitarray(message)\n",
    "        if private_message_bitlen is not None:\n",
    "            output = output[:private_message_bitlen]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
