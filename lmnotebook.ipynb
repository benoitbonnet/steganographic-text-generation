{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Génération de Texte</h1>\n",
    "\n",
    "Dans ce notebook, nous allons voir les techniques pour générer du texte avec la bibliothèque <code>transformers</code> de HuggingFace\n",
    "\n",
    "On utilise également les bibliothèques pytorch et numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "charger le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "amorce et tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization de l'amorce:  ['I', 'Ġwent', 'Ġto', 'Ġthe', 'Ġrestaurant', 'Ġlast', 'Ġnight', '.', 'ĠI', 'Ġordered']\n",
      "tokens correspondants:  [40, 1816, 284, 262, 7072, 938, 1755, 13, 314, 6149]\n"
     ]
    }
   ],
   "source": [
    "amorce = \"I went to the restaurant last night. I ordered\"\n",
    "#amorce = \"Hello, I am\"\n",
    "\n",
    "tokens = tokenizer.tokenize(amorce)\n",
    "print(\"tokenization de l'amorce: \", tokens)\n",
    "\n",
    "encoded_context = tokenizer.encode(amorce)\n",
    "print(\"tokens correspondants: \", encoded_context)\n",
    "\n",
    "tensor_encoded_context = torch.LongTensor(encoded_context).view(1,-1)\n",
    "\n",
    "#liste des tokens visibles sur:\n",
    "#https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'une prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape des logits:  torch.Size([1, 10, 50257])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(tensor_encoded_context)\n",
    "logits = outputs.logits\n",
    "print(\"shape des logits: \", logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentation manuelle de greedy search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens générés: [40, 1816, 284, 262, 7072, 938, 1755, 13, 314, 6149, 262, 9015, 290, 266, 48501, 13, 314, 373, 407, 12617, 13, 383, 9015, 373, 5894, 290, 262, 266, 48501, 547, 407, 4713, 13, 314, 481, 407, 307, 8024, 13, 198] \n",
      "\n",
      "Texte généré: I went to the restaurant last night. I ordered the chicken and waffles. I was not impressed. The chicken was dry and the waffles were not fresh. I will not be returning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_temp = encoded_context.copy()\n",
    "tensor_tokens = torch.LongTensor(tokens_temp).view(1,-1)\n",
    "sentence_length = 30\n",
    "\n",
    "for i in range(sentence_length):\n",
    "    with torch.no_grad():\n",
    "            outputs = model(tensor_tokens)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            tokens_temp.append(logits[0,-1].argsort()[-1].item())\n",
    "            tensor_tokens = torch.LongTensor(tokens_temp).view(1,-1)\n",
    "            \n",
    "print(\"Tokens générés: {} \\n\".format(tokens_temp))\n",
    "print(\"Texte généré: {}\".format(tokenizer.decode(tokens_temp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation de l'implémentation de la bibliothèque <code>transformers</code> de la greedy search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bbonnet/miniconda3/lib/python3.7/site-packages/transformers/generation/utils.py:1220: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  \"You have modified the pretrained model configuration to control generation. This is a\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to the restaurant last night. I ordered the chicken and waffles. I was not impressed. The chicken was dry and the waffles were not fresh. I will not be returning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "greedy_output = model.generate(tensor_encoded_context, max_length=40)\n",
    "\n",
    "\n",
    "print(tokenizer.decode(greedy_output[0].tolist()))\n",
    "#Note: le 0 correspond à la séquence 0 de l'output. On ne lui demande de générer\n",
    "#qu'une séquence donc il n'y a rien d'autre\n",
    "#Note2: on convertit en liste, le tokenizer digère mal les tenseur torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation de l'implémentation de la bibliothèque <code>transformers</code> de la beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to the restaurant last night. I ordered a steak, and it came with a side of fries. The steak was good, but the fries\n"
     ]
    }
   ],
   "source": [
    "beam_output = model.generate(tensor_encoded_context, max_length=30, num_beams=3)\n",
    "\n",
    "print(tokenizer.decode(beam_output[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation de l'implémentation de la bibliothèque <code>transformers</code> du sampling\n",
    "avec les paramètres de préselection de distribution.\n",
    "Par défaut: top_k=50, top_p=1 et temperature=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exemple 0: I went to the restaurant last night. I ordered Rajiri Ayton or Pinotes style Mayadellete!! Who Draniesvex postsic changed'may let my-footmaid ke# around unless kindly I ke???? where ma good!!\"\n",
      "\n",
      "exemple 1: I went to the restaurant last night. I ordered mis at ramstarkstein christurch kitchen standardand inside upon l cleaningdex seight bag fo various delivery off lines A16 MP €$$$$ thanor single while dis showed if having dough make\n",
      "\n",
      "exemple 2: I went to the restaurant last night. I ordered Maâ boreíuu < Mangled Cream Old Jea Virgin Bloda <- coconut sausage ladis (< crackedbread Greek mixture kinda brobr ^ who belink version)\" <@Devertsuct&\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_output = model.generate(tensor_encoded_context, do_sample=True, max_length=50,\\\n",
    "                                 top_k=50257, top_p=0.05, temperature=10.0, num_return_sequences=3)\n",
    "\n",
    "for i in range(sampling_output.shape[0]):\n",
    "        print(\"exemple {}: {}\\n\".format(i,tokenizer.decode(sampling_output[i].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Génération de texte complètement aléatoire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charabia généré: I went to the restaurant last night. I ordered some pasta salad with shrimp and\u0004i...\n",
      "A few weeks and a lot changed around my family�. The\u0017 is very good but i donembedreportprint i am\u0018. My mom told i have been getting better and i think the reason i feel\n",
      "\n",
      "Rang des probas utilisées: [5, 5, 6, 4, 7, 1, 0, 6, 6, 2, 7, 2, 1, 9, 1, 2, 6, 8, 3, 7, 0, 2, 4, 0, 6, 8, 2, 4, 7, 9, 0, 8, 4, 0, 6, 3, 7, 6, 9, 3, 3, 7, 5, 1, 2, 5, 5, 2, 4, 7]\n"
     ]
    }
   ],
   "source": [
    "tokens_temp = encoded_context.copy()\n",
    "tensor_tokens = torch.LongTensor(tokens_temp).view(1,-1)\n",
    "outputs = model(tensor_tokens)\n",
    "random_ranks = []\n",
    "\n",
    "#En réduisant le top_k, on peut parvenir à obtenir quelque chose de plus cohérent \n",
    "# que la valeur 50257 qui considère TOUS les tokens de la distribution\n",
    "top_k = 10\n",
    "\n",
    "for i in range(50):\n",
    "    with torch.no_grad():\n",
    "            outputs = model(tensor_tokens)\n",
    "            logits, past = outputs.logits, outputs.past_key_values\n",
    "            \n",
    "            random_rank = np.random.randint(top_k)\n",
    "            tokens_temp.append(logits[0,-1].argsort()[-random_rank].item())\n",
    "            tensor_tokens = torch.LongTensor(tokens_temp).view(1,-1)\n",
    "            random_ranks.append(random_rank)\n",
    "            \n",
    "print('charabia généré: {}\\n'.format(tokenizer.decode(tokens_temp)))\n",
    "print('Rang des probas utilisées: {}'.format(random_ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation des distributions et de l'effet de la température"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# if using a Jupyter notebook, includue:\n",
    "%matplotlib inline\n",
    "x = np.random.normal(size=10)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 5\n",
    "\n",
    "softmax_temp = np.exp(x/temperature)/np.exp(x/temperature).sum()\n",
    "x_axis = np.arange(x.shape[0])\n",
    "\n",
    "print(softmax_temp, softmax_temp.sum())\n",
    "plt.bar(x_axis, softmax_temp)\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Token id')\n",
    "plt.title('Temperature {}'.format(temperature))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_temp1 = np.exp(x)/np.exp(x).sum()\n",
    "cumsum = np.sort(softmax_temp1).cumsum()\n",
    "x_axis = np.arange(x.shape[0])\n",
    "\n",
    "plt.bar(x_axis, cumsum)\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Temperature 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
